"""
Qwen2-Audio Stage 1 è®­ç»ƒ - æ¢¯åº¦æµæµ‹è¯•æˆåŠŸç‰ˆæœ¬

å…³é”®å‘ç°ï¼š
=========
é—®é¢˜æ ¹æºï¼šä½¿ç”¨äº†é”™è¯¯çš„ processor å‚æ•°å
- âŒ é”™è¯¯ï¼šprocessor(text=text, audios=[audio])  # 'audios' (å¤æ•°)
- âœ… æ­£ç¡®ï¼šprocessor(text=text, audio=audio)     # 'audio' (å•æ•°)

ä½¿ç”¨ 'audios' æ—¶ï¼š
  - processor ä¼šå¿½ç•¥è¯¥å‚æ•°ï¼ˆæœ‰è­¦å‘Šä¿¡æ¯ï¼‰
  - è¿”å›çš„ inputs ä¸­æ²¡æœ‰ 'input_features'ï¼ˆéŸ³é¢‘ç‰¹å¾ï¼‰
  - æ¨¡å‹åªå¤„ç†æ–‡æœ¬ï¼Œaudio_tower å’Œ projector ä¸ä¼šè¢«è°ƒç”¨
  - è‡ªç„¶æ²¡æœ‰æ¢¯åº¦æµåˆ°éŸ³é¢‘ç»„ä»¶

ä½¿ç”¨ 'audio' æ—¶ï¼š
  - processor æ­£ç¡®å¤„ç†éŸ³é¢‘æ•°æ®
  - è¿”å›çš„ inputs åŒ…å« 'input_features' å’Œ 'feature_attention_mask'
  - æ¨¡å‹æ­£å¸¸å¤„ç†éŸ³é¢‘ï¼Œè°ƒç”¨ audio_tower å’Œ projector
  - æ¢¯åº¦æ­£ç¡®æµåˆ°éŸ³é¢‘ç»„ä»¶ âœ…

è®­ç»ƒç­–ç•¥ï¼š
=========
Stage 1 ç›®æ ‡ï¼šè®­ç»ƒéŸ³é¢‘ç»„ä»¶ï¼Œå†»ç»“è¯­è¨€æ¨¡å‹
- audio_tower (Qwen2AudioEncoder): å¯è®­ç»ƒ âœ…
- multi_modal_projector (Audio Adapter): å¯è®­ç»ƒ âœ…
- language_model (Qwen2ForCausalLM): å†»ç»“ â„ï¸

å®ç°æ–¹å¼ï¼š
- åªéœ€å°† language_model çš„æ‰€æœ‰å‚æ•°è®¾ä¸º requires_grad=False
- audio_tower å’Œ multi_modal_projector ä¿æŒå¯è®­ç»ƒ
- æ¢¯åº¦ä¼šæ­£ç¡®åœ°åªæµå‘æœªå†»ç»“çš„ç»„ä»¶
"""

import torch
from transformers import AutoProcessor, Qwen2AudioForConditionalGeneration
import numpy as np

print("=" * 100)
print("  Qwen2-Audio Stage 1 æ¢¯åº¦æµæµ‹è¯• - æˆåŠŸç‰ˆæœ¬")
print("=" * 100)

MODEL_NAME = "Qwen/Qwen2-Audio-7B-Instruct"

# ============================================================================
# 1. åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨
# ============================================================================
print("\n1. åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨")
print("-" * 100)

model = Qwen2AudioForConditionalGeneration.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float32,
    device_map='cpu',
    trust_remote_code=True
)

processor = AutoProcessor.from_pretrained(
    MODEL_NAME,
    trust_remote_code=True
)

print(f"âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
print(f"  æ€»å‚æ•°é‡: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B")

# ============================================================================
# 2. åº”ç”¨ Stage 1 è®­ç»ƒç­–ç•¥ï¼šåªå†»ç»“ language_model
# ============================================================================
print("\n2. åº”ç”¨ Stage 1 è®­ç»ƒç­–ç•¥")
print("-" * 100)

# å†»ç»“ language_modelï¼ˆåŒ…æ‹¬ embeddingã€transformer layersã€lm_headï¼‰
for param in model.language_model.parameters():
    param.requires_grad = False

# éªŒè¯å†»ç»“ç­–ç•¥
audio_tower_trainable = sum(p.numel() for p in model.audio_tower.parameters() if p.requires_grad)
projector_trainable = sum(p.numel() for p in model.multi_modal_projector.parameters() if p.requires_grad)
llm_trainable = sum(p.numel() for p in model.language_model.parameters() if p.requires_grad)

print(f"å‚æ•°çŠ¶æ€:")
print(f"  audio_tower: {audio_tower_trainable / 1e6:.2f}M å¯è®­ç»ƒ")
print(f"  multi_modal_projector: {projector_trainable / 1e6:.2f}M å¯è®­ç»ƒ")
print(f"  language_model: {llm_trainable / 1e6:.2f}M å¯è®­ç»ƒ (åº”ä¸º 0)")

assert llm_trainable == 0, "language_model åº”è¯¥å®Œå…¨å†»ç»“"
print(f"âœ“ å†»ç»“ç­–ç•¥æ­£ç¡®")

# ============================================================================
# 3. å‡†å¤‡è®­ç»ƒæ•°æ®
# ============================================================================
print("\n3. å‡†å¤‡è®­ç»ƒæ•°æ®")
print("-" * 100)

# æ„å»ºå¤šæ¨¡æ€å¯¹è¯
conversation = [
    {
        'role': 'user',
        'content': [
            {'type': 'audio', 'audio_url': 'test.wav'},
            {'type': 'text', 'text': 'è¯·è½¬å½•'}
        ]
    },
    {
        'role': 'assistant',
        'content': 'ä»Šå¤©å¤©æ°”çœŸå¥½'
    }
]

# åº”ç”¨èŠå¤©æ¨¡æ¿
text = processor.apply_chat_template(
    conversation,
    add_generation_prompt=False,
    tokenize=False
)

print(f"ç”Ÿæˆçš„æ–‡æœ¬æ¨¡æ¿:")
print(f"  é•¿åº¦: {len(text)} å­—ç¬¦")
print(f"  åŒ…å« <|AUDIO|> token: {'<|AUDIO|>' in text}")

# ç”Ÿæˆæ¨¡æ‹ŸéŸ³é¢‘ï¼ˆ3ç§’ï¼Œ16kHzé‡‡æ ·ç‡ï¼‰
audio = np.random.randn(16000 * 3).astype(np.float32)
print(f"\néŸ³é¢‘æ•°æ®:")
print(f"  å½¢çŠ¶: {audio.shape}")
print(f"  é‡‡æ ·ç‡: 16000 Hz")
print(f"  æ—¶é•¿: 3.0 ç§’")

# ============================================================================
# 4. å…³é”®æ­¥éª¤ï¼šä½¿ç”¨æ­£ç¡®çš„å‚æ•°åè°ƒç”¨ processor
# ============================================================================
print("\n4. ä½¿ç”¨ processor å¤„ç†è¾“å…¥")
print("-" * 100)

# âœ… æ­£ç¡®æ–¹å¼ï¼šä½¿ç”¨ 'audio' (å•æ•°)
print("è°ƒç”¨æ–¹å¼: processor(text=text, audio=audio, return_tensors='pt')")
inputs = processor(
    text=text,
    audio=audio,  # âœ… å…³é”®ï¼šä½¿ç”¨ 'audio' è€Œä¸æ˜¯ 'audios'
    return_tensors='pt'
)

print(f"\nè¿”å›çš„ keys:")
for key in inputs.keys():
    if torch.is_tensor(inputs[key]):
        print(f"  {key}: shape={inputs[key].shape}")

# éªŒè¯éŸ³é¢‘ç‰¹å¾æ˜¯å¦å­˜åœ¨
if 'input_features' in inputs:
    print(f"\nâœ“ æˆåŠŸï¼éŸ³é¢‘ç‰¹å¾å·²åŒ…å«åœ¨ inputs ä¸­")
    print(f"  input_features shape: {inputs['input_features'].shape}")
else:
    print(f"\nâœ— è­¦å‘Šï¼šinputs ä¸­æ²¡æœ‰ input_features")
    print(f"  è¿™æ„å‘³ç€éŸ³é¢‘æ•°æ®æ²¡æœ‰è¢«å¤„ç†")

# ============================================================================
# 5. Forward Pass
# ============================================================================
print("\n5. Forward Pass")
print("-" * 100)

# å‡†å¤‡æ ‡ç­¾ï¼ˆç”¨äºè®¡ç®—æŸå¤±ï¼‰
labels = inputs['input_ids'].clone()

# è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼
model.train()

# Forward
outputs = model(
    input_ids=inputs['input_ids'],
    attention_mask=inputs['attention_mask'],
    input_features=inputs.get('input_features'),
    feature_attention_mask=inputs.get('feature_attention_mask'),
    labels=labels
)

print(f"Loss:")
print(f"  å€¼: {outputs.loss.item():.4f}")
print(f"  requires_grad: {outputs.loss.requires_grad}")
print(f"  grad_fn: {outputs.loss.grad_fn}")

# ============================================================================
# 6. Backward Pass
# ============================================================================
print("\n6. Backward Pass")
print("-" * 100)

if outputs.loss.requires_grad:
    outputs.loss.backward()
    print("âœ“ Backward å®Œæˆ")
else:
    print("âœ— é”™è¯¯ï¼šloss ä¸å¯æ±‚å¯¼")
    exit(1)

# ============================================================================
# 7. éªŒè¯æ¢¯åº¦åˆ†å¸ƒ
# ============================================================================
print("\n7. éªŒè¯æ¢¯åº¦åˆ†å¸ƒ")
print("-" * 100)

# ç»Ÿè®¡æ¯ä¸ªç»„ä»¶çš„æ¢¯åº¦æƒ…å†µ
def count_gradients(module, name):
    """ç»Ÿè®¡æ¨¡å—ä¸­æœ‰æ¢¯åº¦çš„å‚æ•°æ•°é‡"""
    total_params = 0
    params_with_grad = 0
    params_with_nonzero_grad = 0

    for param in module.parameters():
        total_params += 1
        if param.grad is not None:
            params_with_grad += 1
            if param.grad.abs().sum() > 0:
                params_with_nonzero_grad += 1

    return total_params, params_with_grad, params_with_nonzero_grad

audio_stats = count_gradients(model.audio_tower, 'audio_tower')
proj_stats = count_gradients(model.multi_modal_projector, 'multi_modal_projector')
llm_stats = count_gradients(model.language_model, 'language_model')

print(f"æ¢¯åº¦ç»Ÿè®¡:")
print(f"\naudio_tower:")
print(f"  æ€»å‚æ•°: {audio_stats[0]}")
print(f"  æœ‰æ¢¯åº¦: {audio_stats[1]}")
print(f"  éé›¶æ¢¯åº¦: {audio_stats[2]}")

print(f"\nmulti_modal_projector:")
print(f"  æ€»å‚æ•°: {proj_stats[0]}")
print(f"  æœ‰æ¢¯åº¦: {proj_stats[1]}")
print(f"  éé›¶æ¢¯åº¦: {proj_stats[2]}")

print(f"\nlanguage_model:")
print(f"  æ€»å‚æ•°: {llm_stats[0]}")
print(f"  æœ‰æ¢¯åº¦: {llm_stats[1]}")
print(f"  éé›¶æ¢¯åº¦: {llm_stats[2]}")

# ============================================================================
# 8. æœ€ç»ˆéªŒè¯
# ============================================================================
print("\n" + "=" * 100)
print("  æœ€ç»ˆéªŒè¯")
print("=" * 100)

success = True
errors = []

# æ£€æŸ¥ 1: audio_tower åº”è¯¥æœ‰æ¢¯åº¦
if audio_stats[2] == 0:
    success = False
    errors.append("audio_tower æ²¡æœ‰æ¢¯åº¦")
else:
    print(f"âœ“ audio_tower æœ‰æ¢¯åº¦ ({audio_stats[2]}/{audio_stats[0]} å‚æ•°)")

# æ£€æŸ¥ 2: multi_modal_projector åº”è¯¥æœ‰æ¢¯åº¦
if proj_stats[2] == 0:
    success = False
    errors.append("multi_modal_projector æ²¡æœ‰æ¢¯åº¦")
else:
    print(f"âœ“ multi_modal_projector æœ‰æ¢¯åº¦ ({proj_stats[2]}/{proj_stats[0]} å‚æ•°)")

# æ£€æŸ¥ 3: language_model ä¸åº”è¯¥æœ‰æ¢¯åº¦
if llm_stats[2] > 0:
    success = False
    errors.append(f"language_model æœ‰æ¢¯åº¦ ({llm_stats[2]} å‚æ•°)")
else:
    print(f"âœ“ language_model æ²¡æœ‰æ¢¯åº¦ (æ­£ç¡®å†»ç»“)")

print("\n" + "=" * 100)
if success:
    print("ğŸ‰ğŸ‰ğŸ‰ æˆåŠŸï¼Stage 1 è®­ç»ƒç­–ç•¥éªŒè¯é€šè¿‡")
    print("=" * 100)
    print("\nå…³é”®è¦ç‚¹æ€»ç»“:")
    print("1. ä½¿ç”¨æ­£ç¡®çš„å‚æ•°å: audio (å•æ•°) è€Œä¸æ˜¯ audios (å¤æ•°)")
    print("2. åªå†»ç»“ language_modelï¼Œä¿æŒ audio_tower å’Œ projector å¯è®­ç»ƒ")
    print("3. æ¢¯åº¦æ­£ç¡®æµå‘éŸ³é¢‘ç»„ä»¶ï¼Œä¸ä¼šæµå‘å†»ç»“çš„è¯­è¨€æ¨¡å‹")
    print("4. è¿™å°±æ˜¯ Qwen2-Audio Stage 1 è®­ç»ƒçš„æ­£ç¡®æ–¹å¼")
else:
    print("âŒ éªŒè¯å¤±è´¥")
    print("=" * 100)
    for error in errors:
        print(f"  - {error}")
    exit(1)

print("\n" + "=" * 100)
print("  è°ƒæŸ¥å†ç¨‹å›é¡¾")
print("=" * 100)

print("""
é—®é¢˜å‘ç°è¿‡ç¨‹ï¼š
1. åˆå§‹é—®é¢˜ï¼šRuntimeError: element 0 of tensors does not require grad
2. å°è¯•å¤šç§å†»ç»“ç­–ç•¥ï¼šå…¨éƒ¨å¤±è´¥
3. æµ‹è¯•ç®€åŒ–æ¨¡å‹ï¼šæˆåŠŸ â†’ è¯´æ˜ PyTorch æœºåˆ¶æ²¡é—®é¢˜
4. æ·±å…¥ transformers æºç ï¼šmasked_scatter æ”¯æŒæ¢¯åº¦
5. æ£€æŸ¥ processor è¾“å…¥ï¼šå‘ç°ä½¿ç”¨äº†é”™è¯¯çš„å‚æ•°å 'audios'
6. ä¿®æ­£ä¸º 'audio'ï¼šé—®é¢˜è§£å†³ âœ…

æ•™è®­ï¼š
- ä»”ç»†æ£€æŸ¥ API æ–‡æ¡£å’Œå‚æ•°å
- ä½¿ç”¨é”™è¯¯çš„å‚æ•°åå¯èƒ½å¯¼è‡´æ•°æ®è¢«é™é»˜å¿½ç•¥
- å½“æ¨¡å‹è¡Œä¸ºå¼‚å¸¸æ—¶ï¼Œæ£€æŸ¥è¾“å…¥æ•°æ®æ˜¯å¦æ­£ç¡®ä¼ é€’
- ç®€åŒ–æ¨¡å‹æµ‹è¯•æ˜¯æ’æŸ¥é—®é¢˜çš„æœ‰æ•ˆæ–¹æ³•
""")

print("=" * 100)
