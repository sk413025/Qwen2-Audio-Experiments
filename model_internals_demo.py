"""
Qwen2-Audio æ¨¡å‹å…§éƒ¨è™•ç†æµç¨‹æ·±å…¥è§£æ
å±•ç¤ºæ–‡å­—å’ŒéŸ³é »å¦‚ä½•åœ¨æ¨¡å‹å…§éƒ¨é€šéä¸åŒ encoder è™•ç†ä¸¦èåˆ
"""

from io import BytesIO
from urllib.request import urlopen
import librosa
import torch
from transformers import Qwen2AudioForConditionalGeneration, AutoProcessor
import numpy as np

def print_section(title):
    """æ‰“å°ç« ç¯€æ¨™é¡Œ"""
    print("\n" + "=" * 100)
    print(f"  {title}")
    print("=" * 100)

def print_subsection(title):
    """æ‰“å°å­ç« ç¯€æ¨™é¡Œ"""
    print(f"\n--- {title} " + "-" * (95 - len(title)))

def print_ascii_architecture():
    """é¡¯ç¤ºæ¨¡å‹æ¶æ§‹çš„ ASCII åœ–ç¤º"""
    print_section("Qwen2-Audio æ¨¡å‹æ¶æ§‹åœ–")

    architecture = """

    è¼¸å…¥éšæ®µï¼šå…©å€‹ç¨ç«‹çš„è™•ç†ç®¡é“
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚   æ–‡å­—è¼¸å…¥       â”‚                    â”‚   éŸ³é »è¼¸å…¥       â”‚
         â”‚  "é€™æ˜¯ä»€éº¼è²éŸ³ï¼Ÿ" â”‚                    â”‚ audio.mp3 (æ³¢å½¢) â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚                                       â”‚
                  â”‚ Tokenization                          â”‚ Feature Extraction
                  â”‚ (BPE)                                 â”‚ (Whisper-based)
                  â–¼                                       â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  Token IDs       â”‚                    â”‚ Mel-Spectrogram â”‚
         â”‚ [100792, 98297,  â”‚                    â”‚  [1, 128, 3000] â”‚
         â”‚  99672, ...]     â”‚                    â”‚  é€£çºŒæµ®é»æ•¸ç‰¹å¾µ  â”‚
         â”‚  é›¢æ•£æ•´æ•¸åºåˆ—     â”‚                    â”‚                 â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚                                       â”‚
                  â”‚                                       â”‚
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


    åµŒå…¥éšæ®µï¼šè½‰æ›ç‚ºç›¸åŒç¶­åº¦çš„å‘é‡ç©ºé–“
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                  â”‚                                       â”‚
                  â”‚ Token Embedding                       â”‚ Audio Encoder
                  â”‚ Lookup Table                          â”‚ (Transformer-based)
                  â–¼                                       â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚ Text Embeddings  â”‚                    â”‚ Audio Embeddingsâ”‚
         â”‚ [batch, seq_len, â”‚                    â”‚ [batch, T, dim] â”‚
         â”‚  hidden_dim]     â”‚                    â”‚                 â”‚
         â”‚ ä¾‹: [1, 32, 3584]â”‚                    â”‚ ä¾‹: [1, 187, 3584]â”‚
         â”‚                  â”‚                    â”‚                 â”‚
         â”‚ æ¯å€‹ token è®Šæˆ  â”‚                    â”‚ æ¯å€‹æ™‚é–“å¹€è®Šæˆ   â”‚
         â”‚ 3584 ç¶­å‘é‡      â”‚                    â”‚ 3584 ç¶­å‘é‡     â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚                                       â”‚
                  â”‚                                       â”‚
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


    èåˆéšæ®µï¼šåˆä½µæˆçµ±ä¸€çš„åºåˆ—
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                  â”‚                                       â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â”‚ Sequence Concatenation
                               â”‚ (æ ¹æ“š <|AUDIO|> ç‰¹æ®Š token ä½ç½®æ’å…¥)
                               â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   çµ±ä¸€çš„ Embedding   â”‚
                    â”‚   åºåˆ—               â”‚
                    â”‚                      â”‚
                    â”‚ [ç‰¹æ®Štoken] [æ–‡å­—]   â”‚
                    â”‚ [<|audio_bos|>]     â”‚
                    â”‚ [<|AUDIO|>]         â”‚  â† éŸ³é » embeddings æ’å…¥é€™è£¡
                    â”‚ [<|audio_eos|>]     â”‚
                    â”‚ [é€™][æ˜¯][ä»€éº¼]...    â”‚
                    â”‚                      â”‚
                    â”‚ å½¢ç‹€: [batch,        â”‚
                    â”‚  total_seq_len, dim] â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


    Transformer è™•ç†éšæ®µï¼šè·¨æ¨¡æ…‹æ³¨æ„åŠ›
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                               â”‚
                               â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  QwenLM Transformer  â”‚
                    â”‚  (32+ å±¤)            â”‚
                    â”‚                      â”‚
                    â”‚  Multi-Head          â”‚
                    â”‚  Self-Attention      â”‚
                    â”‚  â†“                   â”‚
                    â”‚  æ–‡å­— tokens å¯ä»¥    â”‚
                    â”‚  attend åˆ° éŸ³é »      â”‚
                    â”‚  â†•                   â”‚
                    â”‚  éŸ³é » embeddings å¯ä»¥â”‚
                    â”‚  attend åˆ° æ–‡å­—      â”‚
                    â”‚  â†“                   â”‚
                    â”‚  Feed-Forward        â”‚
                    â”‚  Network             â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Contextualized      â”‚
                    â”‚  Representations     â”‚
                    â”‚  æ¯å€‹ä½ç½®éƒ½åŒ…å«äº†     â”‚
                    â”‚  æ–‡å­—å’ŒéŸ³é »çš„ä¸Šä¸‹æ–‡   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•


    è¼¸å‡ºéšæ®µï¼šç”Ÿæˆæ–‡å­—å›æ‡‰
    â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

                               â”‚
                               â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Language Head       â”‚
                    â”‚  (Linear Layer)      â”‚
                    â”‚                      â”‚
                    â”‚  [batch, seq_len,    â”‚
                    â”‚   vocab_size]        â”‚
                    â”‚  ä¾‹: [1, seq, 151643]â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â”‚ Softmax + Sampling
                               â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Generated Token IDs â”‚
                    â”‚  [101234, 98765, ...]â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â”‚ Decoding
                               â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  ç”Ÿæˆçš„æ–‡å­—å›æ‡‰       â”‚
                    â”‚  "é€™æ˜¯ç»ç’ƒç ´ç¢çš„è²éŸ³" â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    """
    print(architecture)

def demonstrate_internal_flow():
    """å¯¦éš›å±•ç¤ºæ¨¡å‹å…§éƒ¨çš„è™•ç†æµç¨‹"""

    print_ascii_architecture()

    print_section("æ­¥é©Ÿ 1: è¼‰å…¥æ¨¡å‹å’Œ Processor")

    processor = AutoProcessor.from_pretrained("Qwen/Qwen2-Audio-7B-Instruct")
    model = Qwen2AudioForConditionalGeneration.from_pretrained(
        "Qwen/Qwen2-Audio-7B-Instruct",
        device_map="auto",
        torch_dtype="auto"
    )

    print(f"âœ“ æ¨¡å‹è¼‰å…¥å®Œæˆ")
    print(f"  - æ¨¡å‹é¡å‹: {type(model).__name__}")
    print(f"  - Tokenizer: {type(processor.tokenizer).__name__}")
    print(f"  - Feature Extractor: {type(processor.feature_extractor).__name__}")

    # ç²å–æ¨¡å‹çš„éš±è—å±¤ç¶­åº¦
    hidden_size = model.config.text_config.hidden_size
    print(f"  - éš±è—å±¤ç¶­åº¦: {hidden_size}")

    print_section("æ­¥é©Ÿ 2: æº–å‚™è¼¸å…¥æ•¸æ“š")

    # æº–å‚™æ–‡å­—
    text_input = "é€™æ˜¯ä»€éº¼è²éŸ³ï¼Ÿ"
    print(f"\nğŸ“ æ–‡å­—è¼¸å…¥: \"{text_input}\"")

    # æº–å‚™éŸ³é »
    audio_url = "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3"
    print(f"\nğŸµ éŸ³é »è¼¸å…¥: {audio_url}")
    print("   ä¸‹è¼‰ä¸­...")

    audio_data = urlopen(audio_url).read()
    audio, sr = librosa.load(
        BytesIO(audio_data),
        sr=processor.feature_extractor.sampling_rate
    )

    print(f"   âœ“ éŸ³é »è¼‰å…¥å®Œæˆ")
    print(f"     - æ¡æ¨£ç‡: {sr} Hz")
    print(f"     - æ¨£æœ¬æ•¸: {len(audio)}")
    print(f"     - æ™‚é•·: {len(audio)/sr:.2f} ç§’")

    print_section("æ­¥é©Ÿ 3: æ–‡å­—è™•ç†ç®¡é“ - Token IDs (é›¢æ•£)")

    # æ§‹å»ºåŒ…å«éŸ³é »æ¨™è¨˜çš„æç¤º
    conversation = [
        {"role": "user", "content": [
            {"type": "audio", "audio_url": audio_url},
            {"type": "text", "text": text_input},
        ]},
    ]

    text_prompt = processor.apply_chat_template(
        conversation,
        add_generation_prompt=True,
        tokenize=False
    )

    print(f"\næ§‹å»ºçš„æç¤ºæ–‡å­— (åŒ…å«ç‰¹æ®ŠéŸ³é » tokens):")
    print(f"{text_prompt[:300]}...")

    # Tokenization
    text_tokens = processor.tokenizer(text_prompt, return_tensors="pt")

    print(f"\næ–‡å­— Tokenization çµæœ:")
    print(f"  - Token IDs å½¢ç‹€: {text_tokens.input_ids.shape}")
    print(f"  - Token IDs é¡å‹: {text_tokens.input_ids.dtype}")
    print(f"  - Token IDs ç¯„ä¾‹ (å‰10å€‹): {text_tokens.input_ids[0][:10].tolist()}")

    # æ‰¾å‡ºç‰¹æ®ŠéŸ³é » tokens
    audio_bos_id = processor.tokenizer.convert_tokens_to_ids("<|audio_bos|>")
    audio_id = processor.tokenizer.convert_tokens_to_ids("<|AUDIO|>")
    audio_eos_id = processor.tokenizer.convert_tokens_to_ids("<|audio_eos|>")

    print(f"\nç‰¹æ®ŠéŸ³é » Token IDs:")
    print(f"  - <|audio_bos|>: {audio_bos_id}")
    print(f"  - <|AUDIO|>: {audio_id}")
    print(f"  - <|audio_eos|>: {audio_eos_id}")

    # æ‰¾å‡ºé€™äº› tokens åœ¨åºåˆ—ä¸­çš„ä½ç½®
    token_ids = text_tokens.input_ids[0]
    audio_positions = []
    for i, tid in enumerate(token_ids):
        if tid == audio_id:
            audio_positions.append(i)
            print(f"  - <|AUDIO|> å‡ºç¾åœ¨ä½ç½®: {i}")

    print_section("æ­¥é©Ÿ 4: éŸ³é »è™•ç†ç®¡é“ - é€£çºŒç‰¹å¾µ")

    # Audio feature extraction
    audio_features = processor.feature_extractor(
        audio,
        sampling_rate=sr,
        return_tensors="pt"
    )

    print(f"\néŸ³é »ç‰¹å¾µæå–çµæœ:")
    print(f"  - ç‰¹å¾µå½¢ç‹€: {audio_features.input_features.shape}")
    print(f"    [batch_size, mel_channels, time_frames]")
    print(f"  - ç‰¹å¾µé¡å‹: {audio_features.input_features.dtype}")
    print(f"  - æ•¸å€¼ç¯„åœ: [{audio_features.input_features.min():.4f}, {audio_features.input_features.max():.4f}]")
    print(f"  - å¹³å‡å€¼: {audio_features.input_features.mean():.4f}")

    # é¡¯ç¤ºç‰¹å¾µçš„ä¸€éƒ¨åˆ†
    print(f"\néŸ³é »ç‰¹å¾µçŸ©é™£çš„ä¸€å°éƒ¨åˆ† (å‰3å€‹é€šé“, å‰5å€‹æ™‚é–“å¹€):")
    feature_sample = audio_features.input_features[0, :3, :5].numpy()
    for i, row in enumerate(feature_sample):
        print(f"  Channel {i}: " + " ".join([f"{v:7.3f}" for v in row]))

    print_section("æ­¥é©Ÿ 5: çµ±ä¸€è™•ç† - åˆä½µæ–‡å­—å’ŒéŸ³é »")

    # ä½¿ç”¨ processor çµ±ä¸€è™•ç†
    audios = [audio]
    inputs = processor(text=text_prompt, audios=audios, return_tensors="pt", padding=True)

    print(f"\nçµ±ä¸€è™•ç†å¾Œçš„è¼¸å…¥:")
    print(f"  - input_ids å½¢ç‹€: {inputs.input_ids.shape}")
    print(f"    [batch_size, sequence_length]")
    print(f"  - attention_mask å½¢ç‹€: {inputs.attention_mask.shape}")

    if 'input_features' in inputs:
        print(f"  - input_features å½¢ç‹€: {inputs.input_features.shape}")
        print(f"    [batch_size, mel_channels, time_frames]")
    else:
        print(f"  âš ï¸  input_features æœªåŒ…å« (å¯èƒ½æ˜¯ç‰ˆæœ¬å•é¡Œ)")

    print_section("æ­¥é©Ÿ 6: æ¨¡å‹å…§éƒ¨è™•ç† (ç†è«–æµç¨‹)")

    print("""
    ç”±æ–¼ç›´æ¥è¨ªå•æ¨¡å‹å…§éƒ¨éœ€è¦æ·±å…¥ transformers æºç¢¼ï¼Œé€™è£¡è§£é‡‹ç†è«–æµç¨‹ï¼š

    6.1 Token Embedding éšæ®µ
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    è¼¸å…¥: input_ids [1, seq_len] (é›¢æ•£æ•´æ•¸)
    è™•ç†: model.get_input_embeddings()(input_ids)
    è¼¸å‡º: text_embeddings [1, seq_len, hidden_dim]

    ç¯„ä¾‹å½¢ç‹€è®ŠåŒ–:
      [1, 32] â†’ [1, 32, 3584]
      æ¯å€‹ token ID æŸ¥è¡¨å¾—åˆ° 3584 ç¶­çš„åµŒå…¥å‘é‡


    6.2 Audio Encoding éšæ®µ
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    è¼¸å…¥: input_features [1, 128, time_frames] (é€£çºŒæµ®é»æ•¸)
    è™•ç†: model.audio_encoder(input_features)
           â†“
         Audio Transformer layers
           â†“
         Linear projection to hidden_dim
    è¼¸å‡º: audio_embeddings [1, T, hidden_dim]

    ç¯„ä¾‹å½¢ç‹€è®ŠåŒ–:
      [1, 128, 3000] â†’ [1, 187, 3584]
      Mel-spectrogram â†’ éŸ³é »åµŒå…¥å‘é‡åºåˆ—
      (æ³¨æ„: time_frames 3000 è¢«ä¸‹æ¡æ¨£åˆ° 187 å€‹ embeddings)


    6.3 Embedding èåˆéšæ®µ
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    è™•ç†æµç¨‹:
      1. æ‰¾åˆ° <|AUDIO|> token çš„ä½ç½® (ä¾‹å¦‚ä½ç½® 15)
      2. å°‡ audio_embeddings æ’å…¥åˆ°è©²ä½ç½®
      3. èª¿æ•´ attention_mask ä»¥åŒ…å«éŸ³é »é•·åº¦

    èåˆå¾Œçš„åºåˆ—:
      [<|im_start|>] [system] [You] [are] ...
      [<|audio_bos|>]
      [<|AUDIO|>] â† é€™è£¡è¢«æ›¿æ›æˆ audio_embeddings [1, 187, 3584]
      [<|audio_eos|>]
      [é€™][æ˜¯][ä»€éº¼][è²éŸ³][ï¼Ÿ]
      [<|im_end|>]
      ...

    æœ€çµ‚å½¢ç‹€: [1, seq_len + audio_len, hidden_dim]
    ä¾‹å¦‚: [1, 32 + 187, 3584] = [1, 219, 3584]


    6.4 Transformer è™•ç†éšæ®µ
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    è¼¸å…¥: èåˆå¾Œçš„ embeddings [1, 219, 3584]

    åœ¨æ¯ä¸€å±¤ Transformer ä¸­:
      a) Multi-Head Self-Attention
         - æ–‡å­— tokens å¯ä»¥ attend åˆ°éŸ³é » embeddings
         - éŸ³é » embeddings å¯ä»¥ attend åˆ°æ–‡å­— tokens
         - å¯¦ç¾è·¨æ¨¡æ…‹ä¿¡æ¯äº¤æ›

      b) Feed-Forward Network
         - éç·šæ€§è®Šæ›
         - é€²ä¸€æ­¥æ•´åˆä¿¡æ¯

    è¼¸å‡º: ç¶“é 32 å±¤å¾Œçš„ contextualized embeddings [1, 219, 3584]

    æ¯å€‹ä½ç½®çš„å‘é‡ç¾åœ¨åŒ…å«äº†:
      - è©²ä½ç½®è‡ªå·±çš„ä¿¡æ¯
      - ä¾†è‡ªå…¶ä»–æ–‡å­— tokens çš„ä¸Šä¸‹æ–‡
      - ä¾†è‡ªéŸ³é »å…§å®¹çš„ä¿¡æ¯


    6.5 Language Head ç”Ÿæˆéšæ®µ
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    è¼¸å…¥: contextualized embeddings [1, 219, 3584]
    è™•ç†: Linear(hidden_dim â†’ vocab_size)
    è¼¸å‡º: logits [1, 219, 151643]

    å°æ–¼ç”Ÿæˆä»»å‹™:
      - åªå–æœ€å¾Œä¸€å€‹ä½ç½®çš„ logits
      - æ‡‰ç”¨ softmax å¾—åˆ°æ¦‚ç‡åˆ†å¸ƒ
      - æ¡æ¨£æˆ–é¸æ“‡æœ€é«˜æ¦‚ç‡çš„ token
      - å°‡æ–° token æ·»åŠ åˆ°åºåˆ—ï¼Œç¹¼çºŒç”Ÿæˆ
    """)

    print_section("æ­¥é©Ÿ 7: å¯¦éš›ç”Ÿæˆå›æ‡‰")

    print("\næ­£åœ¨ç”Ÿæˆå›æ‡‰...")
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    with torch.no_grad():
        # ç”Ÿæˆ
        generate_ids = model.generate(
            **inputs,
            max_length=256,
            do_sample=False  # ä½¿ç”¨ greedy decoding ä»¥ä¿è­‰å¯é‡ç¾æ€§
        )

        # åªä¿ç•™æ–°ç”Ÿæˆçš„ tokens
        generated_tokens = generate_ids[:, inputs['input_ids'].size(1):]

    response = processor.batch_decode(
        generated_tokens,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False
    )[0]

    print(f"\nâœ“ ç”Ÿæˆå®Œæˆ")
    print(f"  - ç”Ÿæˆçš„ token æ•¸é‡: {generated_tokens.size(1)}")
    print(f"  - ç”Ÿæˆçš„ Token IDs: {generated_tokens[0].tolist()}")
    print(f"\nğŸ¤– æ¨¡å‹å›æ‡‰: {response}")

    print_section("æ­¥é©Ÿ 8: é—œéµè¦é»ç¸½çµ")

    summary = """
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                         ğŸ¯ é—œéµè¦é»ç¸½çµ                                      â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    1ï¸âƒ£  å…©ç¨®ä¸åŒçš„è¼¸å…¥è¡¨ç¤º:
       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       æ–‡å­—: Token IDs (é›¢æ•£æ•´æ•¸)
             [100792, 98297, 99672, ...]

       éŸ³é »: Mel-spectrogram (é€£çºŒæµ®é»æ•¸)
             [[-0.758, -0.400, 0.434, ...],
              [-0.720, -0.303, 0.531, ...],
              ...]


    2ï¸âƒ£  å…©ç¨®ä¸åŒçš„ç·¨ç¢¼å™¨:
       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       æ–‡å­—: Token Embedding (æŸ¥æ‰¾è¡¨)
             Token ID â†’ Embedding Vector
             ä¾‹: 100792 â†’ [0.123, -0.456, 0.789, ..., 0.234]  (3584ç¶­)

       éŸ³é »: Audio Encoder (Transformer)
             Mel-spectrogram â†’ Audio Embeddings
             ä¾‹: [128, 3000] â†’ [187, 3584]
             å°‡ 128 é€šé“çš„æ™‚é »ç‰¹å¾µå£“ç¸®ä¸¦ç·¨ç¢¼ç‚ºåºåˆ—


    3ï¸âƒ£  çµ±ä¸€åˆ°ç›¸åŒçš„å‘é‡ç©ºé–“:
       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       ç›®æ¨™: å…©ç¨®æ¨¡æ…‹æœ€çµ‚éƒ½è®Šæˆ [åºåˆ—é•·åº¦, hidden_dim] çš„å½¢ç‹€

       æ–‡å­— embeddings:  [seq_len_text, 3584]
       éŸ³é » embeddings:  [seq_len_audio, 3584]

       é€™æ¨£å®ƒå€‘å°±å¯ä»¥åœ¨åŒä¸€å€‹ Transformer ä¸­è™•ç†ï¼


    4ï¸âƒ£  èåˆæ©Ÿåˆ¶ - åºåˆ—æ‹¼æ¥:
       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       ä¸æ˜¯ç°¡å–®çš„å‘é‡ç›¸åŠ æˆ–æ‹¼æ¥ï¼Œè€Œæ˜¯æŒ‰ç…§ç‰¹æ®Š token çš„ä½ç½®æ’å…¥ï¼š

       åŸå§‹åºåˆ—: [token1] [token2] [<|AUDIO|>] [token3] [token4]
                                      â†“
       æ’å…¥éŸ³é »: [token1] [token2] [audio_emb1] [audio_emb2] ...
                 [audio_embN] [token3] [token4]

       <|AUDIO|> å°±åƒä¸€å€‹"éŒ¨é»"ï¼Œå‘Šè¨´æ¨¡å‹åœ¨é€™è£¡æ’å…¥éŸ³é »å…§å®¹


    5ï¸âƒ£  è·¨æ¨¡æ…‹æ³¨æ„åŠ›:
       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       åœ¨ Transformer çš„ Self-Attention ä¸­:

       â€¢ æ–‡å­— token "ä»€éº¼" å¯ä»¥ attend åˆ°éŸ³é » embeddings
         â†’ ç†è§£éŸ³é »å…§å®¹ä¾†å›ç­”å•é¡Œ

       â€¢ éŸ³é » embedding å¯ä»¥ attend åˆ°æ–‡å­— token "è²éŸ³"
         â†’ çŸ¥é“ç”¨æˆ¶åœ¨å•é—œæ–¼è²éŸ³çš„å•é¡Œ

       â€¢ æ‰€æœ‰ä½ç½®äº’ç›¸äº¤äº’ï¼Œå½¢æˆçµ±ä¸€çš„å¤šæ¨¡æ…‹ç†è§£


    6ï¸âƒ£  ç‚ºä»€éº¼é€™æ¨£è¨­è¨ˆæœ‰æ•ˆ:
       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       âœ“ æ¨¡å¡ŠåŒ–: å„è‡ªçš„ encoder å¯ä»¥ç¨ç«‹é è¨“ç·´
       âœ“ éˆæ´»æ€§: å¯ä»¥è™•ç†ç´”æ–‡å­—ã€ç´”éŸ³é »ã€æˆ–æ··åˆè¼¸å…¥
       âœ“ å¯æ“´å±•: å®¹æ˜“æ·»åŠ æ›´å¤šæ¨¡æ…‹ï¼ˆè¦–é »ã€åœ–åƒç­‰ï¼‰
       âœ“ çµ±ä¸€æ¶æ§‹: ä½¿ç”¨åŒä¸€å€‹ Transformer è™•ç†æ‰€æœ‰æ¨¡æ…‹
       âœ“ ç«¯åˆ°ç«¯: å¯ä»¥è¯åˆè¨“ç·´ï¼Œå„ªåŒ–å¤šæ¨¡æ…‹ç†è§£


    7ï¸âƒ£  èˆ‡å…¶ä»–æ¨¡å‹çš„å°æ¯”:
       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
       é¡ä¼¼çš„æ–¹æ³•:
       â€¢ LLaVA (è¦–è¦º): åœ–åƒ â†’ Vision Encoder â†’ Embeddings â†’ LLM
       â€¢ Whisper (éŸ³é »): éŸ³é » â†’ Audio Encoder â†’ Decoder
       â€¢ Qwen2-Audio: éŸ³é » â†’ Audio Encoder â†’ Embeddings â†’ LLM

       é—œéµå‰µæ–°:
       â€¢ ä½¿ç”¨ Whisper ç´šåˆ¥çš„éŸ³é »ç†è§£
       â€¢ çµåˆå¼·å¤§çš„ Qwen2 èªè¨€æ¨¡å‹
       â€¢ æ”¯æŒå°è©±å¼äº¤äº’å’Œè¤‡é›œæ¨ç†
    """

    print(summary)

if __name__ == "__main__":
    demonstrate_internal_flow()
