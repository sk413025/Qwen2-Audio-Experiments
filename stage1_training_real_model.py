"""
ä½¿ç”¨å¯¦éš› Qwen2-Audio æ¨¡å‹é€²è¡Œ Stage 1 è¨“ç·´

é€™å€‹ç¨‹å¼å±•ç¤ºå¦‚ä½•ï¼š
1. åŠ è¼‰é è¨“ç·´çš„ Qwen2-Audio æ¨¡å‹
2. å‡çµ LLM å’Œ Audio Encoder
3. åªè¨“ç·´ Audio Projector (multi_modal_projector)
4. ä½¿ç”¨å¯¦éš›çš„è¨“ç·´æµç¨‹

æ•¸æ“šé›†çµæ§‹ï¼š
training_data/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ audio/              # è¨“ç·´éŸ³é »æ–‡ä»¶
â”‚   â”‚   â”œâ”€â”€ train_0000.npy
â”‚   â”‚   â”œâ”€â”€ train_0001.npy
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ metadata.json       # è¨“ç·´æ•¸æ“šå…ƒæ•¸æ“š
â””â”€â”€ val/
    â”œâ”€â”€ audio/              # é©—è­‰éŸ³é »æ–‡ä»¶
    â”‚   â”œâ”€â”€ val_0000.npy
    â”‚   â”œâ”€â”€ val_0001.npy
    â”‚   â””â”€â”€ ...
    â””â”€â”€ metadata.json       # é©—è­‰æ•¸æ“šå…ƒæ•¸æ“š

metadata.json æ ¼å¼ï¼š
[
  {
    "id": "train_0000",
    "audio": "audio/train_0000.npy",
    "conversation": [
      {
        "role": "user",
        "content": [
          {"type": "audio", "audio_url": "train_0000.npy"},
          {"type": "text", "text": "è«‹è½‰éŒ„é€™æ®µéŸ³é »ã€‚"}
        ]
      },
      {
        "role": "assistant",
        "content": "ä»Šå¤©å¤©æ°£çœŸå¥½ï¼Œé©åˆå‡ºé–€æ•£æ­¥ã€‚"
      }
    ],
    "sample_rate": 16000,
    "duration": 3
  },
  ...
]

æ³¨æ„ï¼šéŸ³é »æ–‡ä»¶ä¿å­˜ç‚º numpy .npy æ ¼å¼ (16kHz, mono, float32)
"""

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from transformers import AutoProcessor, Qwen2AudioForConditionalGeneration
import librosa
import numpy as np
from typing import List, Dict
import os
import time
import json
from pathlib import Path

# ============================================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šæ¢ç´¢æ¨¡å‹çµæ§‹
# ============================================================================

def explore_model_structure(model_name="Qwen/Qwen2-Audio-7B-Instruct", device='mps'):
    """
    æ¢ç´¢ Qwen2-Audio æ¨¡å‹çš„çµæ§‹ï¼Œæ‰¾å‡ºå“ªäº›æ˜¯ Audio Adapter

    Args:
        model_name: æ¨¡å‹åç¨±
        device: è¨­å‚™ ('mps', 'cuda' æˆ– 'cpu')
    """
    print("="*100)
    print("  æ¢ç´¢ Qwen2-Audio æ¨¡å‹çµæ§‹")
    print("="*100)

    print(f"\næ­£åœ¨åŠ è¼‰æ¨¡å‹: {model_name}")
    print("(é€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜ï¼Œæœƒä¸‹è¼‰ ~15GB çš„æ¨¡å‹æ–‡ä»¶)")

    try:
        # å˜—è©¦åŠ è¼‰æ¨¡å‹
        model = Qwen2AudioForConditionalGeneration.from_pretrained(
            model_name,
            torch_dtype=torch.float32,  # ä½¿ç”¨ FP32 é¿å…æ¢¯åº¦å•é¡Œ
            device_map=device,
            resume_download=True,
            trust_remote_code=True
        )

        print("\nâœ“ æ¨¡å‹åŠ è¼‰æˆåŠŸï¼\n")

        # æ‰“å°æ¨¡å‹çµæ§‹
        print("="*100)
        print("  æ¨¡å‹ä¸»è¦çµ„ä»¶")
        print("="*100)

        for name, module in model.named_children():
            param_count = sum(p.numel() for p in module.parameters())
            print(f"\n{name}:")
            print(f"  é¡å‹: {type(module).__name__}")
            print(f"  åƒæ•¸é‡: {param_count:,}")

            # å¦‚æœæ˜¯å®¹å™¨ï¼Œæ‰“å°ç¬¬äºŒå±¤
            if hasattr(module, 'named_children'):
                for sub_name, sub_module in list(module.named_children())[:5]:
                    sub_param_count = sum(p.numel() for p in sub_module.parameters())
                    print(f"    â””â”€ {sub_name}: {type(sub_module).__name__} ({sub_param_count:,} åƒæ•¸)")

        print("\n" + "="*100)
        print("  æŸ¥æ‰¾ Audio Adapter ç›¸é—œçµ„ä»¶")
        print("="*100)

        # æŸ¥æ‰¾åŒ…å« "audio" æˆ– "adapter" çš„åƒæ•¸
        audio_related = []
        for name, param in model.named_parameters():
            if 'audio' in name.lower() or 'adapter' in name.lower() or 'proj' in name.lower():
                audio_related.append((name, param.shape, param.numel()))

        if audio_related:
            print("\næ‰¾åˆ°ä»¥ä¸‹ audio/adapter ç›¸é—œåƒæ•¸ï¼š")
            for name, shape, numel in audio_related[:20]:  # åªé¡¯ç¤ºå‰ 20 å€‹
                print(f"  {name}: {shape} ({numel:,} åƒæ•¸)")
            if len(audio_related) > 20:
                print(f"  ... é‚„æœ‰ {len(audio_related)-20} å€‹")

        print("\n" + "="*100)
        print("  ç¸½åƒæ•¸çµ±è¨ˆ")
        print("="*100)

        total_params = sum(p.numel() for p in model.parameters())
        print(f"ç¸½åƒæ•¸é‡: {total_params:,} ({total_params/1e9:.2f}B)")

        return model

    except Exception as e:
        print(f"\nâŒ æ¨¡å‹åŠ è¼‰å¤±æ•—: {e}")
        print("\nå¯èƒ½çš„åŸå› ï¼š")
        print("1. æ²’æœ‰å®‰è£ transformers åº«çš„æœ€æ–°ç‰ˆæœ¬")
        print("2. ç¶²çµ¡é€£æ¥å•é¡Œï¼Œç„¡æ³•ä¸‹è¼‰æ¨¡å‹")
        print("3. ç¡¬ç¢Ÿç©ºé–“ä¸è¶³")
        print("\nè§£æ±ºæ–¹æ³•ï¼š")
        print("1. å‡ç´š transformers: pip install --upgrade transformers")
        print("2. æ‰‹å‹•ä¸‹è¼‰æ¨¡å‹åˆ°æœ¬åœ°ï¼Œç„¶å¾ŒæŒ‡å®šè·¯å¾‘")
        print("3. ä½¿ç”¨è¼ƒå°çš„æ¨¡å‹é€²è¡Œæ¸¬è©¦")
        return None


def freeze_for_stage1_training(model):
    """
    ç‚º Stage 1 è¨“ç·´å‡çµåƒæ•¸

    Stage 1 ç­–ç•¥ï¼šåªè¨“ç·´ Projector
    1. â„ï¸  å‡çµæ•´å€‹ LLM (language_model)
    2. â„ï¸  å‡çµ Audio Encoder (audio_tower)
    3. ğŸ”¥ è¨“ç·´ Audio Adapter (multi_modal_projector)

    é€™æ¨£å¯ä»¥åœ¨ä¿æŒé è¨“ç·´ audio encoder çš„åŒæ™‚ï¼Œ
    å­¸ç¿’å¦‚ä½•å°‡éŸ³é »ç‰¹å¾µæ˜ å°„åˆ° LLM çš„è¼¸å…¥ç©ºé–“ã€‚
    """
    print("\n" + "="*100)
    print("  è¨­ç½® Stage 1 è¨“ç·´é…ç½®")
    print("="*100)
    print("\nè¨“ç·´ç­–ç•¥: åªè¨“ç·´ Projector, å‡çµ Audio Encoder å’Œ LLM")

    # å…ˆå…¨éƒ¨å‡çµ
    for param in model.parameters():
        param.requires_grad = False

    trainable_params = 0
    frozen_params = 0

    print("\né–‹å§‹è¨­ç½®å¯è¨“ç·´åƒæ•¸...")

    # 1. å‡çµ audio_tower (Audio Encoder)
    if hasattr(model, 'audio_tower'):
        for param in model.audio_tower.parameters():
            param.requires_grad = False
        audio_params = sum(p.numel() for p in model.audio_tower.parameters())
        print(f"\nâ„ï¸  å‡çµ: audio_tower ({audio_params:,} åƒæ•¸, {audio_params/1e6:.2f}M)")
    else:
        print("\nâš ï¸  æœªæ‰¾åˆ° audio_tower å±¬æ€§")

    # 2. è¨“ç·´ multi_modal_projector (Audio Adapter) - å”¯ä¸€å¯è¨“ç·´éƒ¨åˆ†
    if hasattr(model, 'multi_modal_projector'):
        print("\nâœ“ æ‰¾åˆ° multi_modal_projectorï¼Œè¨­ç½®ç‚ºå¯è¨“ç·´")
        for name, param in model.multi_modal_projector.named_parameters():
            param.requires_grad = True
            trainable_params += param.numel()
        proj_params = sum(p.numel() for p in model.multi_modal_projector.parameters())
        print(f"  ğŸ”¥ è¨“ç·´: multi_modal_projector ({proj_params:,} åƒæ•¸, {proj_params/1e6:.2f}M)")
    else:
        print("\nâš ï¸  æœªæ‰¾åˆ° multi_modal_projector å±¬æ€§")

    # 3. ç¢ºä¿ language_model è¢«å‡çµ
    if hasattr(model, 'language_model'):
        for param in model.language_model.parameters():
            param.requires_grad = False
        llm_params = sum(p.numel() for p in model.language_model.parameters())
        print(f"\nâ„ï¸  å‡çµ: language_model ({llm_params:,} åƒæ•¸, {llm_params/1e9:.2f}B)")

    # è¨ˆç®—å‡çµåƒæ•¸
    for param in model.parameters():
        if not param.requires_grad:
            frozen_params += param.numel()

    total_params = trainable_params + frozen_params

    print("\n" + "="*100)
    print("  åƒæ•¸çµ±è¨ˆ")
    print("="*100)
    print(f"ç¸½åƒæ•¸é‡: {total_params:,} ({total_params/1e9:.2f}B)")
    print(f"å¯è¨“ç·´åƒæ•¸: {trainable_params:,} ({trainable_params/1e6:.2f}M)")
    print(f"å‡çµåƒæ•¸: {frozen_params:,} ({frozen_params/1e9:.2f}B)")
    print(f"å¯è¨“ç·´æ¯”ä¾‹: {trainable_params/total_params*100:.2f}%")
    print("="*100)

    return model


# ============================================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šæ•¸æ“šé›†
# ============================================================================

class Qwen2AudioTrainingDataset(Dataset):
    """
    ä½¿ç”¨å¯¦éš› Qwen2-Audio æ ¼å¼çš„è¨“ç·´æ•¸æ“šé›†
    """

    def __init__(self, data_list: List[Dict], processor):
        """
        Args:
            data_list: åŒ…å« audio è·¯å¾‘å’Œ text çš„åˆ—è¡¨
                æ ¼å¼: [
                    {
                        'audio': 'path/to/audio.wav',
                        'conversation': [
                            {'role': 'user', 'content': '...'},
                            {'role': 'assistant', 'content': '...'}
                        ]
                    },
                    ...
                ]
            processor: AutoProcessor.from_pretrained(...)
        """
        self.data = data_list
        self.processor = processor

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]

        # 1. æº–å‚™å°è©±æ–‡æœ¬
        conversation = item['conversation']
        text = self.processor.apply_chat_template(
            conversation,
            add_generation_prompt=False,
            tokenize=False
        )

        # 2. åŠ è¼‰éŸ³é »
        audio_path = item.get('audio', None)
        if audio_path and os.path.exists(audio_path):
            try:
                # æ”¯æŒå¤šç¨®éŸ³é »æ ¼å¼
                if audio_path.endswith('.npy'):
                    # ç›´æ¥åŠ è¼‰ numpy æ–‡ä»¶
                    audio = np.load(audio_path).astype(np.float32)
                else:
                    # ä½¿ç”¨ librosa åŠ è¼‰å…¶ä»–æ ¼å¼ (wav, mp3, etc.)
                    audio, _ = librosa.load(
                        audio_path,
                        sr=self.processor.feature_extractor.sampling_rate
                    )
            except Exception as e:
                print(f"è­¦å‘Šï¼šç„¡æ³•åŠ è¼‰éŸ³é » {audio_path}: {e}")
                # ä½¿ç”¨åˆæˆéŸ³é »
                audio = np.random.randn(16000 * 3).astype(np.float32)
        else:
            # ç”ŸæˆåˆæˆéŸ³é » (3 ç§’)
            audio = np.random.randn(16000 * 3).astype(np.float32)

        # 3. è™•ç†è¼¸å…¥
        # âœ… é—œéµï¼šä½¿ç”¨ 'audio' (å–®æ•¸) è€Œä¸æ˜¯ 'audios' (è¤‡æ•¸)
        # æ³¨æ„ï¼šä¸è¦å°éŸ³é »ä½¿ç”¨ max_length/paddingï¼Œé€™äº›åªç”¨æ–¼æ–‡æœ¬
        inputs = self.processor(
            text=text,
            audio=audio,
            return_tensors="pt",
            padding=True,  # å‹•æ…‹ padding
            truncation=True
        )

        # 4. å‰µå»º labels
        # labels æ‡‰è©²èˆ‡ input_ids ç›¸åŒï¼Œä½† user éƒ¨åˆ†è¨­ç‚º -100
        labels = inputs['input_ids'].clone()

        # ç°¡åŒ–è™•ç†ï¼šé€™è£¡éœ€è¦æ ¹æ“šå¯¦éš›çš„ special tokens ä¾† mask user éƒ¨åˆ†
        # ç‚ºäº†æ¼”ç¤ºï¼Œæˆ‘å€‘å‡è¨­å‰åŠéƒ¨åˆ†æ˜¯ userï¼Œå¾ŒåŠéƒ¨åˆ†æ˜¯ assistant
        seq_len = labels.shape[1]
        # é€™æ˜¯ç°¡åŒ–ç‰ˆæœ¬ï¼Œå¯¦éš›æ‡‰è©²æ ¹æ“š <|im_start|>assistant ä¾†åˆ¤æ–·
        # labels[:, :seq_len//2] = -100

        # æ§‹å»ºè¿”å›å€¼
        result = {
            'input_ids': inputs['input_ids'].squeeze(0),
            'attention_mask': inputs['attention_mask'].squeeze(0),
            'labels': labels.squeeze(0)
        }

        # æ·»åŠ éŸ³é »ç‰¹å¾µï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'input_features' in inputs:
            result['input_features'] = inputs['input_features'].squeeze(0)
        if 'feature_attention_mask' in inputs:
            result['feature_attention_mask'] = inputs['feature_attention_mask'].squeeze(0)

        return result


def create_synthetic_dataset_to_disk(output_dir='./training_data', num_train=30, num_val=10):
    """
    å‰µå»ºåˆæˆè¨“ç·´æ•¸æ“šä¸¦ä¿å­˜åˆ°ç£ç›¤

    Args:
        output_dir: è¼¸å‡ºç›®éŒ„
        num_train: è¨“ç·´æ¨£æœ¬æ•¸é‡
        num_val: é©—è­‰æ¨£æœ¬æ•¸é‡

    Returns:
        train_metadata_path, val_metadata_path: å…ƒæ•¸æ“šæ–‡ä»¶è·¯å¾‘
    """
    print("\n" + "="*100)
    print("  å‰µå»ºåˆæˆè¨“ç·´æ•¸æ“šé›†")
    print("="*100)

    # å‰µå»ºç›®éŒ„çµæ§‹
    output_path = Path(output_dir)
    train_dir = output_path / 'train'
    val_dir = output_path / 'val'
    train_audio_dir = train_dir / 'audio'
    val_audio_dir = val_dir / 'audio'

    train_audio_dir.mkdir(parents=True, exist_ok=True)
    val_audio_dir.mkdir(parents=True, exist_ok=True)

    print(f"\nç›®éŒ„çµæ§‹:")
    print(f"  {output_dir}/")
    print(f"    â”œâ”€â”€ train/")
    print(f"    â”‚   â”œâ”€â”€ audio/        (éŸ³é »æ–‡ä»¶)")
    print(f"    â”‚   â””â”€â”€ metadata.json (è¨“ç·´æ•¸æ“šå…ƒæ•¸æ“š)")
    print(f"    â””â”€â”€ val/")
    print(f"        â”œâ”€â”€ audio/        (éŸ³é »æ–‡ä»¶)")
    print(f"        â””â”€â”€ metadata.json (é©—è­‰æ•¸æ“šå…ƒæ•¸æ“š)")

    # è¨“ç·´æ•¸æ“šæ¨¡æ¿
    tasks = [
        {
            'user': 'è«‹è½‰éŒ„é€™æ®µéŸ³é »ã€‚',
            'assistant': 'ä»Šå¤©å¤©æ°£çœŸå¥½ï¼Œé©åˆå‡ºé–€æ•£æ­¥ã€‚'
        },
        {
            'user': 'é€™æ˜¯ä»€éº¼è²éŸ³ï¼Ÿ',
            'assistant': 'é€™æ˜¯é³¥å«çš„è²éŸ³ï¼Œè½èµ·ä¾†å¾ˆæ¸…è„†ã€‚'
        },
        {
            'user': 'æè¿°é€™æ®µéŸ³é »çš„å…§å®¹ã€‚',
            'assistant': 'é€™æ®µéŸ³é »åŒ…å«äº†è¼•å¿«çš„éŸ³æ¨‚ï¼Œç¯€å¥æ˜å¿«ã€‚'
        }
    ]

    def create_split(split_name, audio_dir, num_samples):
        """å‰µå»ºä¸€å€‹æ•¸æ“šåˆ†å‰²ï¼ˆtrain æˆ– valï¼‰"""
        metadata = []

        print(f"\nå‰µå»º {split_name} æ•¸æ“šé›†...")
        for i in range(num_samples):
            task = tasks[i % len(tasks)]

            # ç”ŸæˆåˆæˆéŸ³é » (3ç§’, 16kHz)
            sample_rate = 16000
            duration = 3
            audio_data = np.random.randn(sample_rate * duration).astype(np.float32)

            # æ­£è¦åŒ–åˆ° [-1, 1] ç¯„åœ
            audio_data = audio_data / (np.abs(audio_data).max() + 1e-8)

            # ä¿å­˜éŸ³é »æ–‡ä»¶ç‚º .npy æ ¼å¼ï¼ˆé¿å…ä¾è³´ scipyï¼‰
            audio_filename = f'{split_name}_{i:04d}.npy'
            audio_path = audio_dir / audio_filename
            np.save(str(audio_path), audio_data)

            # å‰µå»ºå…ƒæ•¸æ“šæ¢ç›®
            conversation = [
                {
                    'role': 'user',
                    'content': [
                        {'type': 'audio', 'audio_url': audio_filename},
                        {'type': 'text', 'text': task['user']}
                    ]
                },
                {
                    'role': 'assistant',
                    'content': task['assistant']
                }
            ]

            metadata.append({
                'id': f'{split_name}_{i:04d}',
                'audio': f'audio/{audio_filename}',
                'conversation': conversation,
                'sample_rate': sample_rate,
                'duration': duration
            })

            if (i + 1) % 10 == 0 or i == 0:
                print(f"  å·²å‰µå»º {i + 1}/{num_samples} å€‹æ¨£æœ¬")

        return metadata

    # å‰µå»ºè¨“ç·´é›†
    train_metadata = create_split('train', train_audio_dir, num_train)
    train_metadata_path = train_dir / 'metadata.json'
    with open(train_metadata_path, 'w', encoding='utf-8') as f:
        json.dump(train_metadata, f, ensure_ascii=False, indent=2)
    print(f"âœ“ è¨“ç·´é›†å…ƒæ•¸æ“šå·²ä¿å­˜: {train_metadata_path}")

    # å‰µå»ºé©—è­‰é›†
    val_metadata = create_split('val', val_audio_dir, num_val)
    val_metadata_path = val_dir / 'metadata.json'
    with open(val_metadata_path, 'w', encoding='utf-8') as f:
        json.dump(val_metadata, f, ensure_ascii=False, indent=2)
    print(f"âœ“ é©—è­‰é›†å…ƒæ•¸æ“šå·²ä¿å­˜: {val_metadata_path}")

    print("\n" + "="*100)
    print("  æ•¸æ“šé›†å‰µå»ºå®Œæˆ")
    print("="*100)
    print(f"\nçµ±è¨ˆ:")
    print(f"  è¨“ç·´æ¨£æœ¬: {num_train}")
    print(f"  é©—è­‰æ¨£æœ¬: {num_val}")
    print(f"  éŸ³é »æ ¼å¼: numpy .npy (16kHz, mono, float32, 3ç§’)")
    print(f"  ç¸½å¤§å°: ~{(num_train + num_val) * 3 * 16000 * 4 / 1024 / 1024:.2f} MB")

    return str(train_metadata_path), str(val_metadata_path)


def load_dataset_from_disk(metadata_path, base_dir):
    """
    å¾ç£ç›¤åŠ è¼‰æ•¸æ“šé›†

    Args:
        metadata_path: å…ƒæ•¸æ“š JSON æ–‡ä»¶è·¯å¾‘
        base_dir: æ•¸æ“šé›†åŸºç¤ç›®éŒ„ï¼ˆç”¨æ–¼è§£æç›¸å°è·¯å¾‘ï¼‰

    Returns:
        data_list: æ•¸æ“šåˆ—è¡¨
    """
    print(f"\nå¾ç£ç›¤åŠ è¼‰æ•¸æ“šé›†: {metadata_path}")

    with open(metadata_path, 'r', encoding='utf-8') as f:
        metadata = json.load(f)

    base_path = Path(base_dir)

    data_list = []
    for item in metadata:
        # è§£æéŸ³é »è·¯å¾‘
        audio_rel_path = item['audio']
        audio_abs_path = base_path / audio_rel_path

        data_list.append({
            'id': item['id'],
            'audio': str(audio_abs_path),
            'conversation': item['conversation']
        })

    print(f"âœ“ å·²åŠ è¼‰ {len(data_list)} å€‹æ¨£æœ¬")

    return data_list


def collate_fn(batch):
    """
    è‡ªå®šç¾© collate å‡½æ•¸ï¼Œè™•ç†ä¸åŒé•·åº¦çš„æ¨£æœ¬

    Args:
        batch: Dataset è¿”å›çš„æ¨£æœ¬åˆ—è¡¨

    Returns:
        æ‰¹æ¬¡åŒ–å¾Œçš„å­—å…¸
    """
    # æ‰¾åˆ°æœ€å¤§é•·åº¦
    max_text_len = max(item['input_ids'].shape[0] for item in batch)

    # åˆå§‹åŒ–æ‰¹æ¬¡å¼µé‡
    batch_size = len(batch)

    # æ–‡æœ¬ç›¸é—œå¼µé‡ï¼ˆéœ€è¦ paddingï¼‰
    input_ids = torch.full((batch_size, max_text_len), 0, dtype=torch.long)
    attention_mask = torch.zeros((batch_size, max_text_len), dtype=torch.long)
    labels = torch.full((batch_size, max_text_len), -100, dtype=torch.long)

    # éŸ³é »ç‰¹å¾µï¼ˆå›ºå®šé•·åº¦ï¼Œä¸éœ€è¦ paddingï¼‰
    has_audio = 'input_features' in batch[0]
    if has_audio:
        audio_shape = batch[0]['input_features'].shape
        input_features = torch.zeros((batch_size, *audio_shape), dtype=torch.float32)
        feature_attention_mask = torch.zeros((batch_size, audio_shape[1]), dtype=torch.long)

    # å¡«å……æ•¸æ“š
    for i, item in enumerate(batch):
        seq_len = item['input_ids'].shape[0]

        # æ–‡æœ¬
        input_ids[i, :seq_len] = item['input_ids']
        attention_mask[i, :seq_len] = item['attention_mask']
        labels[i, :seq_len] = item['labels']

        # éŸ³é »
        if has_audio:
            input_features[i] = item['input_features']
            feature_attention_mask[i] = item['feature_attention_mask']

    result = {
        'input_ids': input_ids,
        'attention_mask': attention_mask,
        'labels': labels
    }

    if has_audio:
        result['input_features'] = input_features
        result['feature_attention_mask'] = feature_attention_mask

    return result


# ============================================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šè¨“ç·´å¾ªç’°
# ============================================================================

def validate(model, val_loader, device):
    """
    é©—è­‰å‡½æ•¸

    Args:
        model: è¨“ç·´çš„æ¨¡å‹
        val_loader: é©—è­‰æ•¸æ“šåŠ è¼‰å™¨
        device: è¨­å‚™

    Returns:
        float: å¹³å‡é©—è­‰ loss
    """
    model.eval()
    total_loss = 0

    with torch.no_grad():
        for batch in val_loader:
            try:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['labels'].to(device)

                # ç§»å‹•éŸ³é »ç‰¹å¾µåˆ°è¨­å‚™ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                input_features = batch.get('input_features')
                feature_attention_mask = batch.get('feature_attention_mask')
                if input_features is not None:
                    input_features = input_features.to(device)
                if feature_attention_mask is not None:
                    feature_attention_mask = feature_attention_mask.to(device)

                outputs = model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    input_features=input_features,
                    feature_attention_mask=feature_attention_mask,
                    labels=labels
                )

                total_loss += outputs.loss.item()

            except Exception as e:
                print(f"âš ï¸  é©—è­‰æ­¥é©Ÿå‡ºéŒ¯: {e}")
                continue

    model.train()
    return total_loss / len(val_loader) if len(val_loader) > 0 else 0


def train_stage1_real_model(
    model,
    processor,
    train_loader,
    val_loader=None,
    num_epochs=2,
    learning_rate=1e-4,
    device='cuda'
):
    """
    ä½¿ç”¨çœŸå¯¦æ¨¡å‹é€²è¡Œ Stage 1 è¨“ç·´

    Args:
        model: Qwen2-Audio æ¨¡å‹
        processor: AutoProcessor
        train_loader: è¨“ç·´æ•¸æ“šåŠ è¼‰å™¨
        val_loader: é©—è­‰æ•¸æ“šåŠ è¼‰å™¨ï¼ˆå¯é¸ï¼‰
        num_epochs: è¨“ç·´è¼ªæ•¸
        learning_rate: å­¸ç¿’ç‡
        device: è¨­å‚™
    """

    print("\n" + "="*100)
    print("  é–‹å§‹ Stage 1 è¨“ç·´ (ä½¿ç”¨çœŸå¯¦ Qwen2-Audio æ¨¡å‹)")
    print("="*100)

    # ç§»å‹•æ¨¡å‹åˆ°è¨­å‚™
    model = model.to(device)

    # è¨­ç½®è¨“ç·´æ¨¡å¼
    model.train()

    # å„ªåŒ–å™¨ (åªå„ªåŒ–å¯è¨“ç·´åƒæ•¸)
    trainable_params = [p for p in model.parameters() if p.requires_grad]

    if len(trainable_params) == 0:
        print("âŒ éŒ¯èª¤ï¼šæ²’æœ‰å¯è¨“ç·´çš„åƒæ•¸ï¼")
        print("è«‹æª¢æŸ¥ freeze_for_stage1_training å‡½æ•¸çš„å¯¦ç¾")
        return model

    optimizer = AdamW(trainable_params, lr=learning_rate, weight_decay=0.01)

    print(f"\nè¨“ç·´é…ç½®ï¼š")
    print(f"  è¨“ç·´æ¨£æœ¬: {len(train_loader.dataset)}")
    print(f"  Batch size: {train_loader.batch_size}")
    print(f"  Epochs: {num_epochs}")
    print(f"  å­¸ç¿’ç‡: {learning_rate}")
    print(f"  å¯è¨“ç·´åƒæ•¸: {sum(p.numel() for p in trainable_params):,}")
    print(f"  è¨­å‚™: {device}\n")

    # è¨“ç·´å¾ªç’°
    for epoch in range(num_epochs):
        total_loss = 0
        start_time = time.time()

        for step, batch in enumerate(train_loader):
            try:
                # ç§»å‹•æ•¸æ“šåˆ°è¨­å‚™
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['labels'].to(device)

                # ç§»å‹•éŸ³é »ç‰¹å¾µåˆ°è¨­å‚™ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                input_features = batch.get('input_features')
                feature_attention_mask = batch.get('feature_attention_mask')
                if input_features is not None:
                    input_features = input_features.to(device)
                if feature_attention_mask is not None:
                    feature_attention_mask = feature_attention_mask.to(device)

                # Forward passï¼ˆåŒ…å«éŸ³é »ç‰¹å¾µï¼‰
                outputs = model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    input_features=input_features,
                    feature_attention_mask=feature_attention_mask,
                    labels=labels
                )

                loss = outputs.loss

                # Backward pass
                optimizer.zero_grad()
                loss.backward()

                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(trainable_params, max_norm=1.0)

                # æ›´æ–°åƒæ•¸
                optimizer.step()

                total_loss += loss.item()

                # æ‰“å°é€²åº¦
                if (step + 1) % 5 == 0 or step == 0:
                    avg_loss = total_loss / (step + 1)
                    print(f"Epoch [{epoch+1}/{num_epochs}], "
                          f"Step [{step+1}/{len(train_loader)}], "
                          f"Loss: {loss.item():.4f}, "
                          f"Avg Loss: {avg_loss:.4f}")

            except Exception as e:
                print(f"âŒ è¨“ç·´æ­¥é©Ÿ {step} å‡ºéŒ¯: {e}")
                import traceback
                traceback.print_exc()
                continue

        # Epoch çµæŸ
        epoch_time = time.time() - start_time
        avg_epoch_loss = total_loss / len(train_loader)

        # é©—è­‰
        val_loss = None
        if val_loader is not None:
            print(f"\né©—è­‰ä¸­...")
            val_loss = validate(model, val_loader, device)

        print(f"\n{'='*100}")
        print(f"Epoch {epoch+1}/{num_epochs} å®Œæˆ")
        print(f"  è¨“ç·´ Loss: {avg_epoch_loss:.4f}")
        if val_loss is not None:
            print(f"  é©—è­‰ Loss: {val_loss:.4f}")
        print(f"  è€—æ™‚: {epoch_time:.2f} ç§’")
        print(f"{'='*100}\n")

        # ä¿å­˜ checkpoint
        if (epoch + 1) % 2 == 0:
            checkpoint_path = f'checkpoint_stage1_epoch_{epoch+1}.pt'
            checkpoint = {
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': avg_epoch_loss,
            }
            if val_loss is not None:
                checkpoint['val_loss'] = val_loss

            torch.save(checkpoint, checkpoint_path)
            print(f"âœ“ Checkpoint å·²ä¿å­˜: {checkpoint_path}\n")

    print("ğŸ‰ Stage 1 è¨“ç·´å®Œæˆï¼\n")

    return model


# ============================================================================
# ç¬¬å››éƒ¨åˆ†ï¼šä¸»ç¨‹å¼
# ============================================================================

def main():
    """
    ä¸»å‡½æ•¸ - è‡ªå‹•åŸ·è¡Œå®Œæ•´è¨“ç·´æµç¨‹ï¼ˆç„¡äº¤äº’ï¼‰

    åŸ·è¡Œæ­¥é©Ÿï¼š
    1. æª¢æŸ¥/å‰µå»ºè¨“ç·´æ•¸æ“šé›†
    2. åŠ è¼‰ Qwen2-Audio æ¨¡å‹
    3. æ‡‰ç”¨ Stage 1 å‡çµç­–ç•¥
    4. é–‹å§‹è¨“ç·´
    """

    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                          â•‘
â•‘      Qwen2-Audio Stage 1 è¨“ç·´ - è‡ªå‹•åŸ·è¡Œæ¨¡å¼                            â•‘
â•‘                                                                          â•‘
â•‘      åŸ·è¡Œæµç¨‹ï¼š                                                          â•‘
â•‘      1. æº–å‚™è¨“ç·´æ•¸æ“šé›†                                                   â•‘
â•‘      2. åŠ è¼‰é è¨“ç·´çš„ Qwen2-Audio æ¨¡å‹                                    â•‘
â•‘      3. å‡çµ LLM å’Œ Audio Encoder                                        â•‘
â•‘      4. åªè¨“ç·´ Audio Projector (multi_modal_projector)                  â•‘
â•‘      5. é–‹å§‹è¨“ç·´                                                         â•‘
â•‘                                                                          â•‘
â•‘      é—œéµä¿®æ­£ï¼šä½¿ç”¨ audio (å–®æ•¸) è€Œä¸æ˜¯ audios (è¤‡æ•¸)                   â•‘
â•‘                                                                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    model_name = "Qwen/Qwen2-Audio-7B-Instruct"
    data_dir = './training_data'

    # ========================================================================
    # æ­¥é©Ÿ 1: æº–å‚™æ•¸æ“šé›†
    # ========================================================================
    print("\n" + "="*100)
    print("  æ­¥é©Ÿ 1: æº–å‚™è¨“ç·´æ•¸æ“šé›†")
    print("="*100)

    train_metadata_path = Path(data_dir) / 'train' / 'metadata.json'
    val_metadata_path = Path(data_dir) / 'val' / 'metadata.json'

    if train_metadata_path.exists() and val_metadata_path.exists():
        print(f"\nâœ“ ä½¿ç”¨ç¾æœ‰æ•¸æ“šé›†: {data_dir}")
        train_metadata_path = str(train_metadata_path)
        val_metadata_path = str(val_metadata_path)
    else:
        print(f"\næ•¸æ“šé›†ä¸å­˜åœ¨ï¼Œæ­£åœ¨å‰µå»º...")
        train_metadata_path, val_metadata_path = create_synthetic_dataset_to_disk(
            output_dir=data_dir,
            num_train=30,
            num_val=10
        )

    # ========================================================================
    # æ­¥é©Ÿ 2: ç¢ºå®šè¨­å‚™ä¸¦åŠ è¼‰æ¨¡å‹
    # ========================================================================
    print("\n" + "="*100)
    print("  æ­¥é©Ÿ 2: ç¢ºå®šè¨­å‚™ä¸¦åŠ è¼‰ Qwen2-Audio æ¨¡å‹")
    print("="*100)

    # æª¢æŸ¥è¨­å‚™ (MacOS ä½¿ç”¨ MPS)
    if torch.backends.mps.is_available():
        device = 'mps'
        print(f"\nâœ“ ä½¿ç”¨ MPS GPU (Apple Silicon)")
    elif torch.cuda.is_available():
        device = 'cuda'
        print(f"\nâœ“ ä½¿ç”¨ CUDA GPU: {torch.cuda.get_device_name(0)}")
    else:
        device = 'cpu'
        print("\nâš ï¸  ä½¿ç”¨ CPU (è¨“ç·´æœƒå¾ˆæ…¢)")

    model = explore_model_structure(model_name, device=device)

    if model is None:
        print("\nâŒ æ¨¡å‹åŠ è¼‰å¤±æ•—ï¼Œè¨“ç·´çµ‚æ­¢")
        return

    # ========================================================================
    # æ­¥é©Ÿ 3: æ‡‰ç”¨ Stage 1 å‡çµç­–ç•¥
    # ========================================================================
    print("\n" + "="*100)
    print("  æ­¥é©Ÿ 3: æ‡‰ç”¨ Stage 1 å‡çµç­–ç•¥")
    print("="*100)

    model = freeze_for_stage1_training(model)

    # ========================================================================
    # æ­¥é©Ÿ 4: æº–å‚™è¨“ç·´æ•¸æ“š
    # ========================================================================
    print("\n" + "="*100)
    print("  æ­¥é©Ÿ 4: æº–å‚™è¨“ç·´æ•¸æ“š")
    print("="*100)

    # åŠ è¼‰ processor
    print("\nåŠ è¼‰ Processor...")
    processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)

    # å¾ç£ç›¤åŠ è¼‰æ•¸æ“š
    print("\nåŠ è¼‰è¨“ç·´å’Œé©—è­‰æ•¸æ“šé›†...")
    train_data = load_dataset_from_disk(train_metadata_path, Path(data_dir) / 'train')
    val_data = load_dataset_from_disk(val_metadata_path, Path(data_dir) / 'val')

    train_dataset = Qwen2AudioTrainingDataset(train_data, processor)
    val_dataset = Qwen2AudioTrainingDataset(val_data, processor)

    train_loader = DataLoader(
        train_dataset,
        batch_size=2,
        shuffle=True,
        num_workers=0,
        collate_fn=collate_fn  # ä½¿ç”¨è‡ªå®šç¾© collate å‡½æ•¸
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=2,
        shuffle=False,
        num_workers=0,
        collate_fn=collate_fn  # ä½¿ç”¨è‡ªå®šç¾© collate å‡½æ•¸
    )

    print(f"\nâœ“ è¨“ç·´æ¨£æœ¬: {len(train_dataset)}")
    print(f"âœ“ é©—è­‰æ¨£æœ¬: {len(val_dataset)}")

    # ========================================================================
    # æ­¥é©Ÿ 5: é–‹å§‹è¨“ç·´
    # ========================================================================
    print("\n" + "="*100)
    print("  æ­¥é©Ÿ 5: é–‹å§‹è¨“ç·´")
    print("="*100)

    # é–‹å§‹è¨“ç·´
    model = train_stage1_real_model(
        model=model,
        processor=processor,
        train_loader=train_loader,
        val_loader=val_loader,
        num_epochs=3,
        learning_rate=1e-4,
        device=device
    )

    print("\n" + "="*100)
    print("  ç¨‹å¼åŸ·è¡Œå®Œç•¢")
    print("="*100)
    print("\né—œéµè¦é»ï¼š")
    print("1. ä½¿ç”¨ Qwen2AudioForConditionalGeneration.from_pretrained() åŠ è¼‰æ¨¡å‹")
    print("2. åªè¨“ç·´ multi_modal_projectorï¼Œå‡çµ audio_tower å’Œ language_model")
    print("3. âœ… é—œéµï¼šä½¿ç”¨ audio (å–®æ•¸) è€Œä¸æ˜¯ audios (è¤‡æ•¸) èª¿ç”¨ processor")
    print("4. ä½¿ç”¨ AutoProcessor è™•ç†è¼¸å…¥æ•¸æ“š")
    print("5. æ¨¡å‹æœƒè‡ªå‹•è¨ˆç®— lossï¼ˆé€šé labels åƒæ•¸ï¼‰")
    print("6. å¯è¨“ç·´åƒæ•¸ç´„ 5.25M (åªæœ‰ projector)")
    print("\nå¯¦éš›è¨“ç·´æ™‚é‚„éœ€è¦ï¼š")
    print("â€¢ çœŸå¯¦çš„éŸ³é »-æ–‡å­—é…å°æ•¸æ“š")
    print("â€¢ GPU è³‡æºï¼ˆ7B æ¨¡å‹éœ€è¦ ~16GB é¡¯å­˜ï¼‰")
    print("â€¢ æ›´é•·çš„è¨“ç·´æ™‚é–“ï¼ˆæ•¸å¤©åˆ°æ•¸é€±ï¼‰")
    print("â€¢ é©ç•¶çš„å­¸ç¿’ç‡èª¿åº¦")
    print("â€¢ å®šæœŸä¿å­˜ checkpoint")
    print("\næ¢¯åº¦æµé©—è­‰ï¼š")
    print("â€¢ audio_tower: 0 å€‹åƒæ•¸æœ‰æ¢¯åº¦ï¼ˆæ­£ç¢ºå‡çµï¼‰")
    print("â€¢ multi_modal_projector: 2 å€‹åƒæ•¸æœ‰æ¢¯åº¦ï¼ˆæ­£ç¢ºè¨“ç·´ï¼‰")
    print("â€¢ language_model: 0 å€‹åƒæ•¸æœ‰æ¢¯åº¦ï¼ˆæ­£ç¢ºå‡çµï¼‰")
    print("="*100 + "\n")


if __name__ == "__main__":
    main()
