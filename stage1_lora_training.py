"""
ä½¿ç”¨ LoRA å¾®èª¿ Qwen2-Audio æ¨¡å‹

é€™å€‹ç¨‹å¼å±•ç¤ºæ­£ç¢ºçš„å¾®èª¿æ–¹å¼ï¼š
1. âœ… å‡çµæ‰€æœ‰é è¨“ç·´åƒæ•¸ï¼ˆåŒ…æ‹¬ multi_modal_projectorï¼‰
2. âœ… åªåœ¨ LLM å±¤æ·»åŠ  LoRA adapters
3. âœ… ä¿æŒéŸ³é »-æ–‡æœ¬å°é½Šä¸è¢«ç ´å£
4. âœ… åƒæ•¸é«˜æ•ˆè¨“ç·´

å°æ¯”ï¼š
- âŒ stage1_training_real_model.py: è¨“ç·´ projector â†’ ç ´å£å°é½Š
- âœ… æœ¬è…³æœ¬: ä½¿ç”¨ LoRA â†’ ä¿ç•™å°é½Šï¼Œåªèª¿æ•´ LLM è¡Œç‚º

ä¾è³´å®‰è£ï¼š
pip install peft  # Parameter-Efficient Fine-Tuning library
"""

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from transformers import AutoProcessor, Qwen2AudioForConditionalGeneration
from peft import LoraConfig, get_peft_model, TaskType
import librosa
import numpy as np
from typing import List, Dict
import os
import time
import json
from pathlib import Path

# ============================================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šLoRA é…ç½®
# ============================================================================

def setup_lora_model(model, lora_rank=8, lora_alpha=16, target_modules=None):
    """
    ç‚º Qwen2-Audio è¨­ç½® LoRA

    LoRA (Low-Rank Adaptation) åŸç†ï¼š
    - ä¸ä¿®æ”¹åŸå§‹æ¬Šé‡ W
    - æ·»åŠ ä½ç§©åˆ†è§£ï¼šW' = W + BAï¼ˆå…¶ä¸­ B å’Œ A æ˜¯å°çŸ©é™£ï¼‰
    - åªè¨“ç·´ B å’Œ Aï¼Œåƒæ•¸é‡æ¥µå°

    Args:
        model: Qwen2-Audio æ¨¡å‹
        lora_rank: LoRA ç§©ï¼ˆè¶Šå°åƒæ•¸è¶Šå°‘ï¼Œé€šå¸¸ 4-16ï¼‰
        lora_alpha: LoRA ç¸®æ”¾å› å­ï¼ˆé€šå¸¸ = 2 * rankï¼‰
        target_modules: è¦æ‡‰ç”¨ LoRA çš„æ¨¡çµ„åç¨±

    Returns:
        é…ç½®å¥½ LoRA çš„æ¨¡å‹
    """
    print("\n" + "="*100)
    print("  è¨­ç½® LoRA é…ç½®")
    print("="*100)

    # ğŸ”§ é—œéµä¿®æ­£ï¼šåœ¨æ‡‰ç”¨ LoRA ä¹‹å‰ï¼Œå…ˆæ‰‹å‹•å‡çµ audio_tower å’Œ multi_modal_projector
    print("\næ­¥é©Ÿ 1: å‡çµ audio_tower å’Œ multi_modal_projector")

    if hasattr(model, 'audio_tower'):
        for param in model.audio_tower.parameters():
            param.requires_grad = False
        print("  âœ“ audio_tower å·²å‡çµ")

    if hasattr(model, 'multi_modal_projector'):
        for param in model.multi_modal_projector.parameters():
            param.requires_grad = False
        print("  âœ“ multi_modal_projector å·²å‡çµ")

    # å¦‚æœæ²’æœ‰æŒ‡å®šç›®æ¨™æ¨¡çµ„ï¼Œä½¿ç”¨é»˜èªå€¼ï¼ˆé‡å° Qwen2 çš„ attention å±¤ï¼‰
    if target_modules is None:
        target_modules = [
            "q_proj",  # Query projection
            "k_proj",  # Key projection
            "v_proj",  # Value projection
            "o_proj",  # Output projection
        ]

    print(f"\næ­¥é©Ÿ 2: é…ç½® LoRA")
    print(f"  Rank (r): {lora_rank}")
    print(f"  Alpha: {lora_alpha}")
    print(f"  ç›®æ¨™æ¨¡çµ„: {target_modules}")
    print(f"  é€™äº›æ¨¡çµ„ä½æ–¼: language_model.model.layers[*].self_attn.*")

    # å‰µå»º LoRA é…ç½®
    lora_config = LoraConfig(
        r=lora_rank,                    # LoRA ç§©
        lora_alpha=lora_alpha,          # LoRA ç¸®æ”¾
        target_modules=target_modules,  # ç›®æ¨™æ¨¡çµ„
        lora_dropout=0.05,              # Dropout
        bias="none",                    # ä¸è¨“ç·´ bias
        task_type=TaskType.CAUSAL_LM,   # ä»»å‹™é¡å‹
        modules_to_save=[]              # ä¸é¡å¤–ä¿å­˜ä»»ä½•æ¨¡çµ„
    )

    # æ‡‰ç”¨ LoRA
    print("\næ­¥é©Ÿ 3: æ‡‰ç”¨ LoRA adapters...")
    model = get_peft_model(model, lora_config)

    # ğŸ”§ å†æ¬¡ç¢ºä¿ audio_tower å’Œ multi_modal_projector è¢«å‡çµ
    print("\næ­¥é©Ÿ 4: å†æ¬¡ç¢ºèªå‡çµç‹€æ…‹...")
    if hasattr(model, 'base_model'):
        # PEFT åŒ…è£å¾Œï¼ŒåŸå§‹æ¨¡å‹åœ¨ base_model.model ä¸­
        base = model.base_model.model
        if hasattr(base, 'audio_tower'):
            for param in base.audio_tower.parameters():
                param.requires_grad = False
        if hasattr(base, 'multi_modal_projector'):
            for param in base.multi_modal_projector.parameters():
                param.requires_grad = False
        print("  âœ“ å·²ç¢ºèªå‡çµç‹€æ…‹")

    # æ‰“å°åƒæ•¸çµ±è¨ˆ
    print("\n" + "="*100)
    print("  LoRA åƒæ•¸çµ±è¨ˆ")
    print("="*100)

    model.print_trainable_parameters()

    # è©³ç´°çµ±è¨ˆ
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    all_params = sum(p.numel() for p in model.parameters())

    print(f"\nè©³ç´°çµ±è¨ˆ:")
    print(f"  ç¸½åƒæ•¸: {all_params:,} ({all_params/1e9:.2f}B)")
    print(f"  å¯è¨“ç·´åƒæ•¸: {trainable_params:,} ({trainable_params/1e6:.2f}M)")
    print(f"  å¯è¨“ç·´æ¯”ä¾‹: {100 * trainable_params / all_params:.4f}%")

    print("\nâœ… é—œéµå„ªå‹¢:")
    print("  1. multi_modal_projector å®Œå…¨å‡çµ â†’ éŸ³é »-æ–‡æœ¬å°é½Šä¿ç•™")
    print("  2. audio_tower å®Œå…¨å‡çµ â†’ éŸ³é »ç·¨ç¢¼èƒ½åŠ›ä¿ç•™")
    print("  3. language_model åŸå§‹æ¬Šé‡å‡çµ â†’ èªè¨€èƒ½åŠ›ä¿ç•™")
    print("  4. åªè¨“ç·´ LoRA adapters â†’ åƒæ•¸é«˜æ•ˆï¼Œä¸æœƒéºå¿˜")

    print("="*100)

    return model


def verify_lora_setup(model):
    """
    é©—è­‰ LoRA è¨­ç½®æ˜¯å¦æ­£ç¢º

    æª¢æŸ¥ï¼š
    1. multi_modal_projector æ˜¯å¦å®Œå…¨å‡çµ
    2. audio_tower æ˜¯å¦å®Œå…¨å‡çµ
    3. åªæœ‰ LoRA adapters å¯è¨“ç·´
    """
    print("\n" + "="*100)
    print("  é©—è­‰ LoRA è¨­ç½®")
    print("="*100)

    # æŒ‰æ¨¡çµ„åˆ†é¡åƒæ•¸
    projector_trainable = 0
    projector_frozen = 0
    audio_trainable = 0
    audio_frozen = 0
    lora_trainable = 0
    llm_trainable = 0
    llm_frozen = 0

    for name, param in model.named_parameters():
        if 'multi_modal_projector' in name:
            if param.requires_grad:
                projector_trainable += param.numel()
            else:
                projector_frozen += param.numel()
        elif 'audio_tower' in name:
            if param.requires_grad:
                audio_trainable += param.numel()
            else:
                audio_frozen += param.numel()
        elif 'lora' in name.lower():
            if param.requires_grad:
                lora_trainable += param.numel()
        elif 'language_model' in name:
            if param.requires_grad:
                llm_trainable += param.numel()
            else:
                llm_frozen += param.numel()

    print("\nåƒæ•¸ç‹€æ…‹æª¢æŸ¥:")

    # multi_modal_projector
    print(f"\n1. multi_modal_projector:")
    print(f"   å‡çµ: {projector_frozen:,} ({projector_frozen/1e6:.2f}M)")
    print(f"   å¯è¨“ç·´: {projector_trainable:,}")
    if projector_trainable == 0:
        print("   âœ… å®Œå…¨å‡çµ - éŸ³é »-æ–‡æœ¬å°é½Šä¿ç•™")
    else:
        print("   âŒ æœ‰åƒæ•¸å¯è¨“ç·´ - æœƒç ´å£å°é½Šï¼")

    # audio_tower
    print(f"\n2. audio_tower:")
    print(f"   å‡çµ: {audio_frozen:,} ({audio_frozen/1e6:.2f}M)")
    print(f"   å¯è¨“ç·´: {audio_trainable:,}")
    if audio_trainable == 0:
        print("   âœ… å®Œå…¨å‡çµ - éŸ³é »ç·¨ç¢¼èƒ½åŠ›ä¿ç•™")
    else:
        print("   âŒ æœ‰åƒæ•¸å¯è¨“ç·´")

    # language_model (åŸå§‹æ¬Šé‡)
    print(f"\n3. language_model (åŸå§‹æ¬Šé‡):")
    print(f"   å‡çµ: {llm_frozen:,} ({llm_frozen/1e9:.2f}B)")
    print(f"   å¯è¨“ç·´: {llm_trainable:,}")
    if llm_trainable == 0:
        print("   âœ… å®Œå…¨å‡çµ - èªè¨€èƒ½åŠ›ä¿ç•™")
    else:
        print("   âš ï¸  æœ‰åƒæ•¸å¯è¨“ç·´ï¼ˆé LoRAï¼‰")

    # LoRA adapters
    print(f"\n4. LoRA adapters:")
    print(f"   å¯è¨“ç·´: {lora_trainable:,} ({lora_trainable/1e6:.2f}M)")
    if lora_trainable > 0:
        print("   âœ… åªè¨“ç·´é€™äº›åƒæ•¸ - ä¸æœƒç ´å£åŸå§‹æ¨¡å‹")
    else:
        print("   âŒ æ²’æœ‰ LoRA åƒæ•¸ - è¨­ç½®å¤±æ•—")

    print("\n" + "="*100)

    return projector_trainable == 0 and audio_trainable == 0 and lora_trainable > 0


# ============================================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šé‡ç”¨æ•¸æ“šé›†ï¼ˆèˆ‡ stage1_training_real_model.py ç›¸åŒï¼‰
# ============================================================================

class Qwen2AudioTrainingDataset(Dataset):
    """
    ä½¿ç”¨å¯¦éš› Qwen2-Audio æ ¼å¼çš„è¨“ç·´æ•¸æ“šé›†
    """

    def __init__(self, data_list: List[Dict], processor):
        self.data = data_list
        self.processor = processor

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]

        # 1. æº–å‚™å°è©±æ–‡æœ¬
        conversation = item['conversation']
        text = self.processor.apply_chat_template(
            conversation,
            add_generation_prompt=False,
            tokenize=False
        )

        # 2. åŠ è¼‰éŸ³é »
        audio_path = item.get('audio', None)
        if audio_path and os.path.exists(audio_path):
            try:
                if audio_path.endswith('.npy'):
                    audio = np.load(audio_path).astype(np.float32)
                else:
                    audio, _ = librosa.load(
                        audio_path,
                        sr=self.processor.feature_extractor.sampling_rate
                    )
            except Exception as e:
                print(f"è­¦å‘Šï¼šç„¡æ³•åŠ è¼‰éŸ³é » {audio_path}: {e}")
                audio = np.random.randn(16000 * 3).astype(np.float32)
        else:
            audio = np.random.randn(16000 * 3).astype(np.float32)

        # 3. è™•ç†è¼¸å…¥
        inputs = self.processor(
            text=text,
            audio=audio,
            return_tensors="pt",
            padding=True,
            truncation=True
        )

        # 4. å‰µå»º labels
        labels = inputs['input_ids'].clone()

        # æ§‹å»ºè¿”å›å€¼
        result = {
            'input_ids': inputs['input_ids'].squeeze(0),
            'attention_mask': inputs['attention_mask'].squeeze(0),
            'labels': labels.squeeze(0)
        }

        # æ·»åŠ éŸ³é »ç‰¹å¾µ
        if 'input_features' in inputs:
            result['input_features'] = inputs['input_features'].squeeze(0)
        if 'feature_attention_mask' in inputs:
            result['feature_attention_mask'] = inputs['feature_attention_mask'].squeeze(0)

        return result


def load_dataset_from_disk(metadata_path, base_dir):
    """å¾ç£ç›¤åŠ è¼‰æ•¸æ“šé›†"""
    print(f"\nå¾ç£ç›¤åŠ è¼‰æ•¸æ“šé›†: {metadata_path}")

    with open(metadata_path, 'r', encoding='utf-8') as f:
        metadata = json.load(f)

    base_path = Path(base_dir)

    data_list = []
    for item in metadata:
        audio_rel_path = item['audio']
        audio_abs_path = base_path / audio_rel_path

        data_list.append({
            'id': item['id'],
            'audio': str(audio_abs_path),
            'conversation': item['conversation']
        })

    print(f"âœ“ å·²åŠ è¼‰ {len(data_list)} å€‹æ¨£æœ¬")
    return data_list


def collate_fn(batch):
    """è‡ªå®šç¾© collate å‡½æ•¸"""
    max_text_len = max(item['input_ids'].shape[0] for item in batch)
    batch_size = len(batch)

    input_ids = torch.full((batch_size, max_text_len), 0, dtype=torch.long)
    attention_mask = torch.zeros((batch_size, max_text_len), dtype=torch.long)
    labels = torch.full((batch_size, max_text_len), -100, dtype=torch.long)

    has_audio = 'input_features' in batch[0]
    if has_audio:
        audio_shape = batch[0]['input_features'].shape
        input_features = torch.zeros((batch_size, *audio_shape), dtype=torch.float32)
        feature_attention_mask = torch.zeros((batch_size, audio_shape[1]), dtype=torch.long)

    for i, item in enumerate(batch):
        seq_len = item['input_ids'].shape[0]
        input_ids[i, :seq_len] = item['input_ids']
        attention_mask[i, :seq_len] = item['attention_mask']
        labels[i, :seq_len] = item['labels']

        if has_audio:
            input_features[i] = item['input_features']
            feature_attention_mask[i] = item['feature_attention_mask']

    result = {
        'input_ids': input_ids,
        'attention_mask': attention_mask,
        'labels': labels
    }

    if has_audio:
        result['input_features'] = input_features
        result['feature_attention_mask'] = feature_attention_mask

    return result


# ============================================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šè¨“ç·´å¾ªç’°
# ============================================================================

def validate(model, val_loader, device):
    """é©—è­‰å‡½æ•¸"""
    model.eval()
    total_loss = 0

    with torch.no_grad():
        for batch in val_loader:
            try:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['labels'].to(device)

                input_features = batch.get('input_features')
                feature_attention_mask = batch.get('feature_attention_mask')
                if input_features is not None:
                    input_features = input_features.to(device)
                if feature_attention_mask is not None:
                    feature_attention_mask = feature_attention_mask.to(device)

                outputs = model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    input_features=input_features,
                    feature_attention_mask=feature_attention_mask,
                    labels=labels
                )

                total_loss += outputs.loss.item()

            except Exception as e:
                print(f"âš ï¸  é©—è­‰æ­¥é©Ÿå‡ºéŒ¯: {e}")
                continue

    model.train()
    return total_loss / len(val_loader) if len(val_loader) > 0 else 0


def train_with_lora(
    model,
    processor,
    train_loader,
    val_loader=None,
    num_epochs=2,
    learning_rate=1e-4,
    device='cuda'
):
    """
    ä½¿ç”¨ LoRA é€²è¡Œè¨“ç·´

    Args:
        model: é…ç½®å¥½ LoRA çš„ Qwen2-Audio æ¨¡å‹
        processor: AutoProcessor
        train_loader: è¨“ç·´æ•¸æ“šåŠ è¼‰å™¨
        val_loader: é©—è­‰æ•¸æ“šåŠ è¼‰å™¨
        num_epochs: è¨“ç·´è¼ªæ•¸
        learning_rate: å­¸ç¿’ç‡
        device: è¨­å‚™
    """

    print("\n" + "="*100)
    print("  é–‹å§‹ LoRA è¨“ç·´")
    print("="*100)

    # ç§»å‹•æ¨¡å‹åˆ°è¨­å‚™
    model = model.to(device)
    model.train()

    # å„ªåŒ–å™¨ï¼ˆåªå„ªåŒ– LoRA åƒæ•¸ï¼‰
    trainable_params = [p for p in model.parameters() if p.requires_grad]

    if len(trainable_params) == 0:
        print("âŒ éŒ¯èª¤ï¼šæ²’æœ‰å¯è¨“ç·´çš„åƒæ•¸ï¼")
        return model

    optimizer = AdamW(trainable_params, lr=learning_rate, weight_decay=0.01)

    print(f"\nè¨“ç·´é…ç½®ï¼š")
    print(f"  è¨“ç·´æ¨£æœ¬: {len(train_loader.dataset)}")
    print(f"  Batch size: {train_loader.batch_size}")
    print(f"  Epochs: {num_epochs}")
    print(f"  å­¸ç¿’ç‡: {learning_rate}")
    print(f"  å¯è¨“ç·´åƒæ•¸: {sum(p.numel() for p in trainable_params):,}")
    print(f"  è¨­å‚™: {device}\n")

    # è¨“ç·´å¾ªç’°
    for epoch in range(num_epochs):
        total_loss = 0
        start_time = time.time()

        for step, batch in enumerate(train_loader):
            try:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['labels'].to(device)

                input_features = batch.get('input_features')
                feature_attention_mask = batch.get('feature_attention_mask')
                if input_features is not None:
                    input_features = input_features.to(device)
                if feature_attention_mask is not None:
                    feature_attention_mask = feature_attention_mask.to(device)

                # Forward pass
                outputs = model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    input_features=input_features,
                    feature_attention_mask=feature_attention_mask,
                    labels=labels
                )

                loss = outputs.loss

                # Backward pass
                optimizer.zero_grad()
                loss.backward()

                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(trainable_params, max_norm=1.0)

                optimizer.step()

                total_loss += loss.item()

                # æ‰“å°é€²åº¦
                if (step + 1) % 5 == 0 or step == 0:
                    avg_loss = total_loss / (step + 1)
                    print(f"Epoch [{epoch+1}/{num_epochs}], "
                          f"Step [{step+1}/{len(train_loader)}], "
                          f"Loss: {loss.item():.4f}, "
                          f"Avg Loss: {avg_loss:.4f}")

            except Exception as e:
                print(f"âŒ è¨“ç·´æ­¥é©Ÿ {step} å‡ºéŒ¯: {e}")
                import traceback
                traceback.print_exc()
                continue

        # Epoch çµæŸ
        epoch_time = time.time() - start_time
        avg_epoch_loss = total_loss / len(train_loader)

        # é©—è­‰
        val_loss = None
        if val_loader is not None:
            print(f"\né©—è­‰ä¸­...")
            val_loss = validate(model, val_loader, device)

        print(f"\n{'='*100}")
        print(f"Epoch {epoch+1}/{num_epochs} å®Œæˆ")
        print(f"  è¨“ç·´ Loss: {avg_epoch_loss:.4f}")
        if val_loss is not None:
            print(f"  é©—è­‰ Loss: {val_loss:.4f}")
        print(f"  è€—æ™‚: {epoch_time:.2f} ç§’")
        print(f"{'='*100}\n")

        # ä¿å­˜ checkpoint (åªä¿å­˜ LoRA adapters)
        if (epoch + 1) % 2 == 0:
            checkpoint_path = f'checkpoint_lora_epoch_{epoch+1}'
            model.save_pretrained(checkpoint_path)
            print(f"âœ“ LoRA checkpoint å·²ä¿å­˜: {checkpoint_path}\n")

    print("ğŸ‰ LoRA è¨“ç·´å®Œæˆï¼\n")

    return model


# ============================================================================
# ç¬¬å››éƒ¨åˆ†ï¼šä¸»ç¨‹å¼
# ============================================================================

def main():
    """
    ä¸»å‡½æ•¸ - ä½¿ç”¨ LoRA å¾®èª¿ Qwen2-Audio
    """

    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                          â•‘
â•‘      Qwen2-Audio LoRA å¾®èª¿ - æ­£ç¢ºçš„å¾®èª¿æ–¹å¼                            â•‘
â•‘                                                                          â•‘
â•‘      èˆ‡ stage1_training_real_model.py çš„å°æ¯”ï¼š                          â•‘
â•‘                                                                          â•‘
â•‘      âŒ stage1_training_real_model.py:                                  â•‘
â•‘         - è¨“ç·´ multi_modal_projector (5.25M åƒæ•¸)                       â•‘
â•‘         - ç ´å£å·²æœ‰çš„éŸ³é »-æ–‡æœ¬å°é½Š                                       â•‘
â•‘         - åœ¨éš¨æ©Ÿå™ªéŸ³ä¸Šå­¸ç¿’ï¼Œå°è‡´æ€§èƒ½ä¸‹é™                                â•‘
â•‘                                                                          â•‘
â•‘      âœ… æœ¬è…³æœ¬ (LoRA):                                                  â•‘
â•‘         - å‡çµ multi_modal_projector â†’ ä¿ç•™å°é½Š                         â•‘
â•‘         - åªåœ¨ LLM æ·»åŠ  LoRA adapters (~3M åƒæ•¸)                       â•‘
â•‘         - åŸå§‹æ¨¡å‹å®Œå…¨ä¸è®Šï¼Œå¯éš¨æ™‚æ¢å¾©                                  â•‘
â•‘         - åƒæ•¸æ›´å°‘ï¼Œè¨“ç·´æ›´å¿«ï¼Œä¸æœƒéºå¿˜                                  â•‘
â•‘                                                                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    model_name = "Qwen/Qwen2-Audio-7B-Instruct"
    data_dir = './training_data'

    # ========================================================================
    # æ­¥é©Ÿ 1: ç¢ºå®šè¨­å‚™ä¸¦åŠ è¼‰æ¨¡å‹
    # ========================================================================
    print("\n" + "="*100)
    print("  æ­¥é©Ÿ 1: åŠ è¼‰ Qwen2-Audio æ¨¡å‹")
    print("="*100)

    # æª¢æŸ¥è¨­å‚™
    if torch.backends.mps.is_available():
        device = 'mps'
        print(f"\nâœ“ ä½¿ç”¨ MPS GPU (Apple Silicon)")
    elif torch.cuda.is_available():
        device = 'cuda'
        print(f"\nâœ“ ä½¿ç”¨ CUDA GPU: {torch.cuda.get_device_name(0)}")
    else:
        device = 'cpu'
        print("\nâš ï¸  ä½¿ç”¨ CPU")

    print(f"\næ­£åœ¨åŠ è¼‰æ¨¡å‹...")
    model = Qwen2AudioForConditionalGeneration.from_pretrained(
        model_name,
        torch_dtype=torch.float32,
        device_map=device,
        trust_remote_code=True
    )

    print("âœ“ æ¨¡å‹åŠ è¼‰å®Œæˆ")

    # ========================================================================
    # æ­¥é©Ÿ 2: è¨­ç½® LoRA
    # ========================================================================
    print("\n" + "="*100)
    print("  æ­¥é©Ÿ 2: é…ç½® LoRA")
    print("="*100)

    model = setup_lora_model(
        model,
        lora_rank=8,      # å¯èª¿æ•´ï¼š4, 8, 16, 32
        lora_alpha=16,    # é€šå¸¸ = 2 * rank
        target_modules=["q_proj", "v_proj"]  # åªåœ¨ Q å’Œ V æŠ•å½±æ·»åŠ  LoRA
    )

    # é©—è­‰è¨­ç½®
    if not verify_lora_setup(model):
        print("\nâŒ LoRA è¨­ç½®é©—è­‰å¤±æ•—ï¼Œè«‹æª¢æŸ¥é…ç½®")
        return

    # ========================================================================
    # æ­¥é©Ÿ 3: æº–å‚™æ•¸æ“š
    # ========================================================================
    print("\n" + "="*100)
    print("  æ­¥é©Ÿ 3: æº–å‚™è¨“ç·´æ•¸æ“š")
    print("="*100)

    train_metadata_path = Path(data_dir) / 'train' / 'metadata.json'
    val_metadata_path = Path(data_dir) / 'val' / 'metadata.json'

    if not train_metadata_path.exists():
        print(f"\nâŒ æ‰¾ä¸åˆ°è¨“ç·´æ•¸æ“š: {train_metadata_path}")
        print("è«‹å…ˆé‹è¡Œ stage1_training_real_model.py ç”Ÿæˆæ•¸æ“šï¼Œæˆ–æ‰‹å‹•æº–å‚™æ•¸æ“š")
        return

    # åŠ è¼‰ processor
    print("\nåŠ è¼‰ Processor...")
    processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)

    # åŠ è¼‰æ•¸æ“š
    train_data = load_dataset_from_disk(train_metadata_path, Path(data_dir) / 'train')
    val_data = load_dataset_from_disk(val_metadata_path, Path(data_dir) / 'val')

    train_dataset = Qwen2AudioTrainingDataset(train_data, processor)
    val_dataset = Qwen2AudioTrainingDataset(val_data, processor)

    train_loader = DataLoader(
        train_dataset,
        batch_size=2,
        shuffle=True,
        num_workers=0,
        collate_fn=collate_fn
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=2,
        shuffle=False,
        num_workers=0,
        collate_fn=collate_fn
    )

    print(f"\nâœ“ è¨“ç·´æ¨£æœ¬: {len(train_dataset)}")
    print(f"âœ“ é©—è­‰æ¨£æœ¬: {len(val_dataset)}")

    # ========================================================================
    # æ­¥é©Ÿ 4: é–‹å§‹è¨“ç·´
    # ========================================================================
    print("\n" + "="*100)
    print("  æ­¥é©Ÿ 4: é–‹å§‹ LoRA è¨“ç·´")
    print("="*100)

    model = train_with_lora(
        model=model,
        processor=processor,
        train_loader=train_loader,
        val_loader=val_loader,
        num_epochs=3,
        learning_rate=1e-4,
        device=device
    )

    print("\n" + "="*100)
    print("  è¨“ç·´å®Œæˆ")
    print("="*100)
    print("\nâœ… LoRA å¾®èª¿çš„å„ªå‹¢ï¼š")
    print("  1. multi_modal_projector æœªè¢«ä¿®æ”¹ â†’ éŸ³é »-æ–‡æœ¬å°é½Šå®Œæ•´ä¿ç•™")
    print("  2. audio_tower æœªè¢«ä¿®æ”¹ â†’ éŸ³é »ç·¨ç¢¼èƒ½åŠ›ä¸è®Š")
    print("  3. language_model åŸå§‹æ¬Šé‡ä¸è®Š â†’ å¯éš¨æ™‚ç§»é™¤ LoRA æ¢å¾©åŸå§‹æ¨¡å‹")
    print("  4. åªè¨“ç·´ ~3M åƒæ•¸ â†’ æ¯”è¨“ç·´ projector (5.25M) æ›´è¼•é‡")
    print("  5. å¤šå€‹ LoRA adapters å¯ä»¥å…±å­˜ â†’ æ”¯æŒå¤šä»»å‹™")

    print("\nå¦‚ä½•ä½¿ç”¨è¨“ç·´å¥½çš„æ¨¡å‹ï¼š")
    print("  # åŠ è¼‰ LoRA adapters")
    print("  from peft import PeftModel")
    print("  base_model = Qwen2AudioForConditionalGeneration.from_pretrained(...)")
    print("  model = PeftModel.from_pretrained(base_model, 'checkpoint_lora_epoch_2')")
    print("\n  # æ¨ç†")
    print("  outputs = model.generate(...)")
    print("\n  # ç§»é™¤ LoRA æ¢å¾©åŸå§‹æ¨¡å‹")
    print("  model = model.unload()")

    print("="*100 + "\n")


if __name__ == "__main__":
    main()
