"""
ç”Ÿæˆç©ºé–“éŸ³é »å®šä½æ•¸æ“šé›†

ä½¿ç”¨ HRTF æ¨¡æ“¬å¾å–®è²é“éŸ³é »ç”Ÿæˆé›™è€³éŸ³é »
æ•¸æ“šæ ¼å¼èˆ‡ multi_speaker_data ä¸€è‡´
"""

import numpy as np
import json
import os
from pathlib import Path
from tqdm import tqdm
import librosa
from scipy.signal import butter, lfilter
import random


def apply_itd(audio, itd_samples, sr=16000):
    """
    æ‡‰ç”¨ ITD (Interaural Time Difference)

    Args:
        audio: [samples] éŸ³é »ä¿¡è™Ÿ
        itd_samples: ITD (æ¨£æœ¬æ•¸ï¼Œå¯ä»¥æ˜¯å°æ•¸)
        sr: æ¡æ¨£ç‡

    Returns:
        delayed_audio: å»¶é²å¾Œçš„éŸ³é »
    """
    if abs(itd_samples) < 0.01:
        return audio

    # æ•´æ•¸å»¶é²
    int_delay = int(round(itd_samples))

    if int_delay > 0:
        # æ­£å»¶é²ï¼šåœ¨å‰é¢è£œé›¶
        delayed = np.pad(audio, (int_delay, 0), mode='constant')[:-int_delay]
    elif int_delay < 0:
        # è² å»¶é²ï¼šåœ¨å¾Œé¢è£œé›¶
        delayed = np.pad(audio, (0, -int_delay), mode='constant')[-int_delay:]
    else:
        delayed = audio

    return delayed


def apply_ild_filter(audio, ild_db, sr=16000):
    """
    æ‡‰ç”¨ ILD (Interaural Level Difference)

    ILD åœ¨é«˜é »æ›´æ˜é¡¯ï¼ˆé ­éƒ¨é®æ“‹æ•ˆæ‡‰ï¼‰

    Args:
        audio: [samples] éŸ³é »ä¿¡è™Ÿ
        ild_db: ILD (dB)ï¼Œæ­£å€¼è¡¨ç¤ºå¢ç›Šï¼Œè² å€¼è¡¨ç¤ºè¡°æ¸›
        sr: æ¡æ¨£ç‡

    Returns:
        filtered_audio: æ‡‰ç”¨ ILD å¾Œçš„éŸ³é »
    """
    if abs(ild_db) < 0.1:
        return audio

    # æ‡‰ç”¨å¢ç›Š
    linear_gain = 10 ** (ild_db / 20)
    audio_gained = audio * linear_gain

    # å¦‚æœæ˜¯é ç«¯è€³æœµï¼ˆè²  ILDï¼‰ï¼Œæ‡‰ç”¨ä½é€šæ¿¾æ³¢ï¼ˆæ¨¡æ“¬é«˜é »è¡°æ¸›ï¼‰
    if ild_db < 0:
        # æˆªæ­¢é »ç‡éš¨ ILD è®ŠåŒ–
        cutoff_freq = 8000 + ild_db * 200  # è¶Šè² ï¼Œæˆªæ­¢é »ç‡è¶Šä½
        cutoff_freq = np.clip(cutoff_freq, 2000, 8000)

        # è¨­è¨ˆä½é€šæ¿¾æ³¢å™¨
        nyquist = sr / 2
        normalized_cutoff = cutoff_freq / nyquist

        if normalized_cutoff < 1.0:
            b, a = butter(2, normalized_cutoff, btype='low')
            audio_gained = lfilter(b, a, audio_gained)

    return audio_gained


def generate_binaural_audio(mono_audio, angle_deg, sr=16000):
    """
    ä½¿ç”¨ç°¡åŒ– HRTF æ¨¡å‹ç”Ÿæˆé›™è€³éŸ³é »

    Args:
        mono_audio: [samples] å–®è²é“éŸ³é »
        angle_deg: æ–¹ä½è§’ï¼ˆåº¦ï¼‰ï¼Œ-90 (å·¦) åˆ° +90 (å³)
        sr: æ¡æ¨£ç‡

    Returns:
        left_audio: [samples] å·¦è€³éŸ³é »
        right_audio: [samples] å³è€³éŸ³é »
    """
    angle_rad = np.deg2rad(angle_deg)

    # === ITD è¨ˆç®— (Woodworth å…¬å¼) ===
    head_radius = 0.0875  # m
    sound_speed = 343.0   # m/s

    itd_seconds = (head_radius / sound_speed) * (np.sin(angle_rad) + angle_rad)
    itd_samples = itd_seconds * sr

    # === ILD è¨ˆç®— (ç°¡åŒ–æ¨¡å‹) ===
    # ILD èˆ‡ sin(Î¸) æˆæ­£æ¯”ï¼Œæœ€å¤§ç´„ Â±15 dB
    ild_max = 15  # dB
    ild_db = ild_max * np.sin(angle_rad)

    # === æ‡‰ç”¨åˆ°éŸ³é » ===
    if angle_deg >= 0:
        # è²æºåœ¨å³å´
        # å³è€³ï¼šè¿‘ç«¯ï¼Œå¢ç›Šï¼Œç„¡å»¶é²
        # å·¦è€³ï¼šé ç«¯ï¼Œè¡°æ¸›ï¼Œå»¶é²
        right_audio = apply_ild_filter(mono_audio, ild_db / 2, sr)
        right_audio = apply_itd(right_audio, -itd_samples / 2, sr)

        left_audio = apply_ild_filter(mono_audio, -ild_db / 2, sr)
        left_audio = apply_itd(left_audio, itd_samples / 2, sr)
    else:
        # è²æºåœ¨å·¦å´
        left_audio = apply_ild_filter(mono_audio, -ild_db / 2, sr)  # è² çš„è²  = æ­£å¢ç›Š
        left_audio = apply_itd(left_audio, -itd_samples / 2, sr)

        right_audio = apply_ild_filter(mono_audio, ild_db / 2, sr)  # è²  = è¡°æ¸›
        right_audio = apply_itd(right_audio, itd_samples / 2, sr)

    # ç¢ºä¿é•·åº¦ä¸€è‡´
    min_len = min(len(left_audio), len(right_audio))
    left_audio = left_audio[:min_len]
    right_audio = right_audio[:min_len]

    return left_audio, right_audio


def get_direction_description(angle):
    """
    å°‡è§’åº¦è½‰æ›ç‚ºè‡ªç„¶èªè¨€æè¿°

    Args:
        angle: -90 to 90 åº¦

    Returns:
        description: æ–¹å‘æè¿°
    """
    abs_angle = abs(angle)

    if angle > 5:
        direction = "å³å´"
    elif angle < -5:
        direction = "å·¦å´"
    else:
        return "æ­£å‰æ–¹"

    # è§’åº¦æè¿°
    if abs_angle < 15:
        return f"{direction}åå‰æ–¹å¤§ç´„ {abs_angle:.0f} åº¦"
    elif abs_angle < 45:
        return f"{direction}å‰æ–¹å¤§ç´„ {abs_angle:.0f} åº¦"
    elif abs_angle < 75:
        return f"{direction}å¤§ç´„ {abs_angle:.0f} åº¦"
    else:
        return f"{direction}åå´æ–¹å¤§ç´„ {abs_angle:.0f} åº¦"


def generate_synthetic_audio(duration=3.0, sr=16000, audio_type='speech'):
    """
    ç”ŸæˆåˆæˆéŸ³é »ï¼ˆç”¨æ–¼æ¸¬è©¦ï¼‰

    å¯¦éš›ä½¿ç”¨æ™‚æ‡‰è©²è¼‰å…¥çœŸå¯¦éŸ³é »

    Args:
        duration: æ™‚é•·ï¼ˆç§’ï¼‰
        sr: æ¡æ¨£ç‡
        audio_type: éŸ³é »é¡å‹

    Returns:
        audio: [samples] éŸ³é »ä¿¡è™Ÿ
        description: éŸ³é »å…§å®¹æè¿°
    """
    n_samples = int(duration * sr)

    if audio_type == 'speech':
        # æ¨¡æ“¬èªéŸ³ï¼šå¤šå€‹é »ç‡çš„æ­£å¼¦æ³¢
        t = np.linspace(0, duration, n_samples)
        audio = 0
        for freq in [200, 400, 600, 800]:
            audio += 0.2 * np.sin(2 * np.pi * freq * t)

        # æ·»åŠ åŒ…çµ¡
        envelope = np.exp(-3 * t) * (1 + 0.5 * np.sin(2 * np.pi * 2 * t))
        audio *= envelope

        descriptions = ["ç”·æ€§èªªè©±è²", "å¥³æ€§èªªè©±è²", "å°è©±è²"]
        description = random.choice(descriptions)

    elif audio_type == 'music':
        # æ¨¡æ“¬éŸ³æ¨‚
        t = np.linspace(0, duration, n_samples)
        audio = 0
        for freq in [440, 554, 659]:  # A, C#, E (A major chord)
            audio += 0.3 * np.sin(2 * np.pi * freq * t)

        descriptions = ["é‹¼ç´æ¼”å¥", "å‰ä»–å½ˆå¥", "éŸ³æ¨‚æ’­æ”¾"]
        description = random.choice(descriptions)

    elif audio_type == 'environmental':
        # æ¨¡æ“¬ç’°å¢ƒéŸ³
        # ä½¿ç”¨ç™½å™ªéŸ³ + æ¿¾æ³¢
        audio = np.random.randn(n_samples) * 0.1

        # ä½é€šæ¿¾æ³¢
        b, a = butter(4, 0.2)
        audio = lfilter(b, a, audio)

        descriptions = ["é¢¨è²", "æ°´æµè²", "é³¥å«è²", "é›¨è²"]
        description = random.choice(descriptions)

    else:
        raise ValueError(f"Unknown audio type: {audio_type}")

    # æ­¸ä¸€åŒ–
    audio = audio / (np.abs(audio).max() + 1e-8)
    audio = audio * 0.8  # é¿å…å‰Šæ³¢

    return audio, description


def create_dataset(
    output_dir='spatial_audio_data',
    split='train',
    num_samples=30,
    duration=3.0,
    sr=16000,
    seed=42
):
    """
    å‰µå»ºæ•¸æ“šé›†ï¼ˆtrain æˆ– valï¼‰

    Args:
        output_dir: è¼¸å‡ºç›®éŒ„
        split: 'train' or 'val'
        num_samples: æ¨£æœ¬æ•¸é‡
        duration: éŸ³é »æ™‚é•·ï¼ˆç§’ï¼‰
        sr: æ¡æ¨£ç‡
        seed: éš¨æ©Ÿç¨®å­
    """
    np.random.seed(seed)
    random.seed(seed)

    # å‰µå»ºç›®éŒ„
    split_dir = Path(output_dir) / split
    left_dir = split_dir / 'left'
    right_dir = split_dir / 'right'
    mono_dir = split_dir / 'mono'

    for d in [left_dir, right_dir, mono_dir]:
        d.mkdir(parents=True, exist_ok=True)

    metadata = []

    # éŸ³é »é¡å‹åˆ†ä½ˆ
    audio_types = ['speech'] * 12 + ['music'] * 9 + ['environmental'] * 9

    print(f"\nç”Ÿæˆ {split} é›†: {num_samples} å€‹æ¨£æœ¬")

    for i in tqdm(range(num_samples)):
        sample_id = f"{split}_{i:04d}"

        # éš¨æ©Ÿè§’åº¦ï¼ˆ-90 åˆ° +90ï¼‰
        angle = np.random.uniform(-90, 90)

        # éš¨æ©Ÿé¸æ“‡éŸ³é »é¡å‹
        audio_type = random.choice(audio_types)

        # ç”Ÿæˆå–®è²é“éŸ³é »
        mono_audio, audio_description = generate_synthetic_audio(
            duration=duration,
            sr=sr,
            audio_type=audio_type
        )

        # ç”Ÿæˆé›™è€³éŸ³é »
        left_audio, right_audio = generate_binaural_audio(mono_audio, angle, sr)

        # ä¿å­˜éŸ³é »
        np.save(left_dir / f"{sample_id}.npy", left_audio.astype(np.float32))
        np.save(right_dir / f"{sample_id}.npy", right_audio.astype(np.float32))
        np.save(mono_dir / f"{sample_id}.npy", mono_audio.astype(np.float32))

        # æ–¹å‘æè¿°
        direction_desc = get_direction_description(angle)

        # æ§‹å»ºå°è©±
        # å¤šç¨®æç¤ºæ ¼å¼
        prompts = [
            "è«‹æè¿°é€™æ®µéŸ³é »çš„å…§å®¹ä»¥åŠè²æºçš„æ–¹å‘ä½ç½®ã€‚",
            "é€™å€‹è²éŸ³æ˜¯ä»€éº¼ï¼Ÿå®ƒä¾†è‡ªå“ªå€‹æ–¹å‘ï¼Ÿ",
            "åˆ†æé€™æ®µéŸ³é »ï¼ŒåŒ…æ‹¬å…§å®¹å’Œç©ºé–“ä½ç½®ã€‚",
            "è«‹è­˜åˆ¥éŸ³é »å…§å®¹ä¸¦åˆ¤æ–·è²æºæ–¹å‘ã€‚"
        ]
        prompt = random.choice(prompts)

        # å¤šç¨®å›ç­”æ ¼å¼
        responses = [
            f"é€™æ˜¯{audio_description}ï¼Œä¾†è‡ª{direction_desc}ã€‚",
            f"éŸ³é »å…§å®¹æ˜¯{audio_description}ï¼Œè²æºä½æ–¼{direction_desc}ã€‚",
            f"æˆ‘è½åˆ°{audio_description}ï¼Œæ–¹å‘åœ¨{direction_desc}ã€‚",
            f"é€™æ®µéŸ³é »æ˜¯{audio_description}ã€‚å¾å·¦å³è²é“çš„å·®ç•°ä¾†çœ‹ï¼Œè²æºä½æ–¼{direction_desc}ã€‚"
        ]
        response = random.choice(responses)

        # å…ƒæ•¸æ“š
        sample_meta = {
            'id': sample_id,
            'left_audio': f'left/{sample_id}.npy',
            'right_audio': f'right/{sample_id}.npy',
            'mono_audio': f'mono/{sample_id}.npy',
            'angle': float(angle),
            'audio_type': audio_type,
            'audio_description': audio_description,
            'direction_description': direction_desc,
            'conversation': [
                {
                    'role': 'user',
                    'content': prompt
                },
                {
                    'role': 'assistant',
                    'content': response
                }
            ],
            'duration': duration,
            'sample_rate': sr
        }

        metadata.append(sample_meta)

    # ä¿å­˜ metadata
    metadata_path = split_dir / 'metadata.json'
    with open(metadata_path, 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)

    print(f"âœ“ {split} é›†ç”Ÿæˆå®Œæˆ")
    print(f"  - æ¨£æœ¬æ•¸: {num_samples}")
    print(f"  - ä¿å­˜ä½ç½®: {split_dir}")

    return metadata


def generate_statistics(metadata):
    """ç”Ÿæˆæ•¸æ“šé›†çµ±è¨ˆä¿¡æ¯"""
    angles = [s['angle'] for s in metadata]

    stats = {
        'total_samples': len(metadata),
        'angle_range': [min(angles), max(angles)],
        'angle_mean': np.mean(angles),
        'angle_std': np.std(angles),
        'audio_type_distribution': {}
    }

    # çµ±è¨ˆéŸ³é »é¡å‹
    for sample in metadata:
        audio_type = sample['audio_type']
        stats['audio_type_distribution'][audio_type] = \
            stats['audio_type_distribution'].get(audio_type, 0) + 1

    return stats


def create_readme(output_dir, train_stats, val_stats):
    """å‰µå»º README.md"""
    readme_content = f"""# ç©ºé–“éŸ³é »å®šä½æ•¸æ“šé›†
## ä½¿ç”¨ HRTF æ¨¡æ“¬ç”Ÿæˆçš„é›™è€³éŸ³é »

**ç”Ÿæˆæ™‚é–“**: {np.datetime64('now')}
**ä»»å‹™**: å¾é›™è²é“éŸ³é »é æ¸¬è²æºæ–¹å‘

---

## ğŸ“ è³‡æ–™å¤¾çµæ§‹

```
spatial_audio_data/
â”œâ”€â”€ README.md           (æœ¬æ–‡ä»¶)
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ metadata.json   (è¨“ç·´é›†å…ƒæ•¸æ“š)
â”‚   â”œâ”€â”€ left/           (å·¦è²é“éŸ³é » .npy)
â”‚   â”œâ”€â”€ right/          (å³è²é“éŸ³é » .npy)
â”‚   â””â”€â”€ mono/           (æ··åˆéŸ³é » .npy)
â””â”€â”€ val/
    â”œâ”€â”€ metadata.json   (é©—è­‰é›†å…ƒæ•¸æ“š)
    â”œâ”€â”€ left/           (å·¦è²é“éŸ³é » .npy)
    â”œâ”€â”€ right/          (å³è²é“éŸ³é » .npy)
    â””â”€â”€ mono/           (æ··åˆéŸ³é » .npy)
```

---

## ğŸ“Š æ•¸æ“šé›†çµ±è¨ˆ

### è¨“ç·´é›†

- **æ¨£æœ¬æ•¸**: {train_stats['total_samples']}
- **è§’åº¦ç¯„åœ**: [{train_stats['angle_range'][0]:.1f}Â°, {train_stats['angle_range'][1]:.1f}Â°]
- **è§’åº¦å¹³å‡**: {train_stats['angle_mean']:.1f}Â°
- **è§’åº¦æ¨™æº–å·®**: {train_stats['angle_std']:.1f}Â°

**éŸ³é »é¡å‹åˆ†ä½ˆ**:
"""
    for audio_type, count in train_stats['audio_type_distribution'].items():
        percentage = count / train_stats['total_samples'] * 100
        readme_content += f"- {audio_type}: {count} å€‹æ¨£æœ¬ ({percentage:.1f}%)\n"

    readme_content += f"""
### é©—è­‰é›†

- **æ¨£æœ¬æ•¸**: {val_stats['total_samples']}
- **è§’åº¦ç¯„åœ**: [{val_stats['angle_range'][0]:.1f}Â°, {val_stats['angle_range'][1]:.1f}Â°]
- **è§’åº¦å¹³å‡**: {val_stats['angle_mean']:.1f}Â°
- **è§’åº¦æ¨™æº–å·®**: {val_stats['angle_std']:.1f}Â°

**éŸ³é »é¡å‹åˆ†ä½ˆ**:
"""
    for audio_type, count in val_stats['audio_type_distribution'].items():
        percentage = count / val_stats['total_samples'] * 100
        readme_content += f"- {audio_type}: {count} å€‹æ¨£æœ¬ ({percentage:.1f}%)\n"

    readme_content += """
---

## ğŸ“ metadata.json æ ¼å¼

æ¯å€‹æ¨£æœ¬åŒ…å«ä»¥ä¸‹ä¿¡æ¯ï¼š

```json
{
  "id": "train_0000",
  "left_audio": "left/train_0000.npy",
  "right_audio": "right/train_0000.npy",
  "mono_audio": "mono/train_0000.npy",
  "angle": 45.5,
  "audio_type": "speech",
  "audio_description": "ç”·æ€§èªªè©±è²",
  "direction_description": "å³å´å‰æ–¹å¤§ç´„ 46 åº¦",
  "conversation": [
    {
      "role": "user",
      "content": "è«‹æè¿°é€™æ®µéŸ³é »çš„å…§å®¹ä»¥åŠè²æºçš„æ–¹å‘ä½ç½®ã€‚"
    },
    {
      "role": "assistant",
      "content": "é€™æ˜¯ç”·æ€§èªªè©±è²ï¼Œä¾†è‡ªå³å´å‰æ–¹å¤§ç´„ 46 åº¦ã€‚"
    }
  ],
  "duration": 3.0,
  "sample_rate": 16000
}
```

---

## ğŸ” HRTF æ¨¡æ“¬åŸç†

### ITD (Interaural Time Difference)

ä½¿ç”¨ Woodworth å…¬å¼ï¼š
```
ITD = (a/c) Ã— (sin(Î¸) + Î¸)
```
å…¶ä¸­ï¼š
- a = 0.0875 m (é ­éƒ¨åŠå¾‘)
- c = 343 m/s (è²é€Ÿ)
- Î¸ = è§’åº¦ï¼ˆå¼§åº¦ï¼‰

### ILD (Interaural Level Difference)

ç°¡åŒ–æ¨¡å‹ï¼š
```
ILD = 15 Ã— sin(Î¸) dB
```
ä¸¦æ‡‰ç”¨é »ç‡ç›¸é—œçš„ä½é€šæ¿¾æ³¢ï¼ˆæ¨¡æ“¬é ­éƒ¨é®æ“‹ï¼‰

---

## ğŸ”„ é‡æ–°ç”Ÿæˆæ•¸æ“š

```bash
python generate_spatial_audio_dataset.py \\
    --num_train=100 \\
    --num_val=30 \\
    --duration=3.0
```

---

## ğŸ¯ ä½¿ç”¨æ–¹å¼

```python
import numpy as np
import json

# è®€å– metadata
with open('spatial_audio_data/train/metadata.json', 'r') as f:
    metadata = json.load(f)

# è¼‰å…¥æ¨£æœ¬
sample = metadata[0]
left_audio = np.load(f"spatial_audio_data/train/{sample['left_audio']}")
right_audio = np.load(f"spatial_audio_data/train/{sample['right_audio']}")
angle = sample['angle']

print(f"è§’åº¦: {angle}Â°")
print(f"å°è©±: {sample['conversation']}")
```

---

**ç”Ÿæˆå·¥å…·**: generate_spatial_audio_dataset.py
**æ•¸æ“šæ ¼å¼**: NumPy (.npy) + JSON (.json)
"""

    readme_path = Path(output_dir) / 'README.md'
    with open(readme_path, 'w', encoding='utf-8') as f:
        f.write(readme_content)

    print(f"âœ“ README.md å·²å‰µå»º")


def main():
    """ä¸»å‡½æ•¸"""
    print("=" * 80)
    print("  ç”Ÿæˆç©ºé–“éŸ³é »å®šä½æ•¸æ“šé›†")
    print("=" * 80)

    output_dir = 'spatial_audio_data'

    # ç”Ÿæˆè¨“ç·´é›†
    train_metadata = create_dataset(
        output_dir=output_dir,
        split='train',
        num_samples=100,  # 100 å€‹è¨“ç·´æ¨£æœ¬
        duration=3.0,
        sr=16000,
        seed=42
    )

    # ç”Ÿæˆé©—è­‰é›†
    val_metadata = create_dataset(
        output_dir=output_dir,
        split='val',
        num_samples=30,  # 30 å€‹é©—è­‰æ¨£æœ¬
        duration=3.0,
        sr=16000,
        seed=43
    )

    # çµ±è¨ˆä¿¡æ¯
    train_stats = generate_statistics(train_metadata)
    val_stats = generate_statistics(val_metadata)

    print("\n" + "=" * 80)
    print("  æ•¸æ“šé›†çµ±è¨ˆ")
    print("=" * 80)
    print(f"\nè¨“ç·´é›†: {train_stats['total_samples']} å€‹æ¨£æœ¬")
    print(f"  è§’åº¦ç¯„åœ: [{train_stats['angle_range'][0]:.1f}Â°, {train_stats['angle_range'][1]:.1f}Â°]")
    print(f"  éŸ³é »é¡å‹: {train_stats['audio_type_distribution']}")

    print(f"\né©—è­‰é›†: {val_stats['total_samples']} å€‹æ¨£æœ¬")
    print(f"  è§’åº¦ç¯„åœ: [{val_stats['angle_range'][0]:.1f}Â°, {val_stats['angle_range'][1]:.1f}Â°]")
    print(f"  éŸ³é »é¡å‹: {val_stats['audio_type_distribution']}")

    # å‰µå»º README
    create_readme(output_dir, train_stats, val_stats)

    print("\n" + "=" * 80)
    print("  âœ… æ•¸æ“šé›†ç”Ÿæˆå®Œæˆï¼")
    print("=" * 80)
    print(f"\næ•¸æ“šä¿å­˜åœ¨: {output_dir}/")
    print(f"  - è¨“ç·´é›†: {output_dir}/train/")
    print(f"  - é©—è­‰é›†: {output_dir}/val/")


if __name__ == '__main__':
    main()
