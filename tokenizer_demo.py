"""
Qwen2-Audio Tokenizer ä½¿ç”¨ç¯„ä¾‹
å±•ç¤ºæ–‡å­— BPE tokenizer å’ŒéŸ³é » feature extractor çš„è©³ç´°ä½¿ç”¨æ–¹å¼
ä½¿ç”¨ Qwen2-Audio README ä¸­æä¾›çš„ç·šä¸ŠéŸ³æª”
"""

from io import BytesIO
from urllib.request import urlopen
import librosa
import torch
from transformers import Qwen2AudioForConditionalGeneration, AutoProcessor

def main():
    print("=" * 80)
    print("Qwen2-Audio Tokenizer ç¯„ä¾‹ - æ–‡å­—èˆ‡éŸ³é »è™•ç†å±•ç¤º")
    print("=" * 80)

    # ============================================================
    # 1. è¼‰å…¥æ¨¡å‹å’Œ Processor
    # ============================================================
    print("\n[æ­¥é©Ÿ 1] è¼‰å…¥æ¨¡å‹å’Œ Processor...")
    model_name = "Qwen/Qwen2-Audio-7B-Instruct"

    processor = AutoProcessor.from_pretrained(model_name)
    model = Qwen2AudioForConditionalGeneration.from_pretrained(
        model_name,
        device_map="auto",
        torch_dtype="auto"
    )

    print(f"âœ“ æ¨¡å‹è¼‰å…¥å®Œæˆ: {model_name}")
    print(f"âœ“ Tokenizer é¡å‹: {type(processor.tokenizer).__name__}")
    print(f"âœ“ Feature Extractor é¡å‹: {type(processor.feature_extractor).__name__}")

    # ============================================================
    # 2. æ–‡å­— Tokenizer å±•ç¤º (BPE Tokenizer)
    # ============================================================
    print("\n" + "=" * 80)
    print("[æ­¥é©Ÿ 2] æ–‡å­— Tokenizer å±•ç¤º (Byte-level BPE)")
    print("=" * 80)

    # æ¸¬è©¦æ–‡å­—
    test_text = "è«‹å•é€™æ®µéŸ³æª”çš„å…§å®¹æ˜¯ä»€éº¼ï¼Ÿ"

    # ä½¿ç”¨ tokenizer ç·¨ç¢¼æ–‡å­—
    text_tokens = processor.tokenizer(test_text, return_tensors="pt")

    print(f"\nåŸå§‹æ–‡å­—: {test_text}")
    print(f"Token IDs: {text_tokens.input_ids[0].tolist()[:20]}...")  # é¡¯ç¤ºå‰20å€‹
    print(f"Token ç¸½æ•¸: {len(text_tokens.input_ids[0])}")
    print(f"è©å½™è¡¨å¤§å°: {processor.tokenizer.vocab_size:,} tokens")

    # è§£ç¢¼å›æ–‡å­—
    decoded_text = processor.tokenizer.decode(text_tokens.input_ids[0])
    print(f"è§£ç¢¼å¾Œæ–‡å­—: {decoded_text}")

    # é¡¯ç¤º token ç´°ç¯€
    print("\næ–‡å­— Tokenization è©³ç´°è³‡è¨Š:")
    tokens = processor.tokenizer.convert_ids_to_tokens(text_tokens.input_ids[0][:10])
    for i, (token_id, token) in enumerate(zip(text_tokens.input_ids[0][:10], tokens)):
        print(f"  [{i}] ID: {token_id:5d} -> Token: '{token}'")

    # ============================================================
    # 3. éŸ³é »è™•ç†å±•ç¤º (Whisper-based Feature Extractor)
    # ============================================================
    print("\n" + "=" * 80)
    print("[æ­¥é©Ÿ 3] éŸ³é » Feature Extractor å±•ç¤º (Whisper-large-v3 based)")
    print("=" * 80)

    # ä½¿ç”¨ README ä¸­æä¾›çš„ç·šä¸ŠéŸ³æª”
    audio_url = "https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3"

    print(f"\néŸ³æª”ä¾†æº: {audio_url}")
    print("æ­£åœ¨ä¸‹è¼‰éŸ³æª”...")

    # è¼‰å…¥éŸ³é »
    audio_data = urlopen(audio_url).read()
    audio, sr = librosa.load(
        BytesIO(audio_data),
        sr=processor.feature_extractor.sampling_rate
    )

    print(f"âœ“ éŸ³æª”è¼‰å…¥å®Œæˆ")
    print(f"  - æ¡æ¨£ç‡: {processor.feature_extractor.sampling_rate} Hz")
    print(f"  - éŸ³æª”é•·åº¦: {len(audio)} å€‹æ¨£æœ¬")
    print(f"  - éŸ³æª”æ™‚é•·: {len(audio) / processor.feature_extractor.sampling_rate:.2f} ç§’")

    # éŸ³é »ç‰¹å¾µæå–
    print("\néŸ³é »ç‰¹å¾µæå–è¨­å®š:")
    print(f"  - Mel é »è­œé€šé“æ•¸: 128 channels")
    print(f"  - çª—å£å¤§å°: 25ms")
    print(f"  - Hop length: 10ms")

    # è™•ç†éŸ³é »ï¼ˆéœ€è¦æä¾›æ–‡å­—ï¼Œä½¿ç”¨ç°¡å–®çš„éŸ³é »æ¨™è¨˜ï¼‰
    # æ³¨æ„ï¼šQwen2AudioProcessor è¦æ±‚å¿…é ˆåŒæ™‚æä¾›æ–‡å­—å’ŒéŸ³é »
    simple_audio_prompt = "<|audio_bos|><|AUDIO|><|audio_eos|>"
    audio_features = processor(
        text=simple_audio_prompt,
        audios=[audio],
        return_tensors="pt"
    )

    # æª¢æŸ¥è¿”å›çš„å…§å®¹
    print(f"\nè™•ç†å™¨è¿”å›çš„éµ: {audio_features.keys()}")

    if 'input_features' in audio_features:
        print(f"\néŸ³é »ç‰¹å¾µå¼µé‡å½¢ç‹€: {audio_features.input_features.shape}")
        print(f"  - ç¶­åº¦èªªæ˜: [batch_size, mel_channels, time_frames]")

        # é¡¯ç¤ºéŸ³é »ç‰¹å¾µçš„çµ±è¨ˆè³‡è¨Š
        print(f"\néŸ³é »ç‰¹å¾µçµ±è¨ˆ:")
        print(f"  - æœ€å°å€¼: {audio_features.input_features.min().item():.4f}")
        print(f"  - æœ€å¤§å€¼: {audio_features.input_features.max().item():.4f}")
        print(f"  - å¹³å‡å€¼: {audio_features.input_features.mean().item():.4f}")
    else:
        print("\nâš ï¸  æ³¨æ„: éŸ³é »ç‰¹å¾µæœªåŒ…å«åœ¨è¿”å›ä¸­ï¼ˆå¯èƒ½æ˜¯ç‰ˆæœ¬å•é¡Œï¼‰")
        print("    å°‡åœ¨æ­¥é©Ÿ4ä¸­å±•ç¤ºå®Œæ•´çš„éŸ³é »+æ–‡å­—è™•ç†")

    # ============================================================
    # 4. å®Œæ•´å°è©±ç¯„ä¾‹ - æ–‡å­— + éŸ³é »
    # ============================================================
    print("\n" + "=" * 80)
    print("[æ­¥é©Ÿ 4] å®Œæ•´å°è©±ç¯„ä¾‹ - çµåˆæ–‡å­—èˆ‡éŸ³é »")
    print("=" * 80)

    # æ§‹å»ºå°è©±
    conversation = [
        {"role": "user", "content": [
            {"type": "audio", "audio_url": audio_url},
            {"type": "text", "text": "é€™æ˜¯ä»€éº¼è²éŸ³ï¼Ÿ"},
        ]},
    ]

    # æ‡‰ç”¨èŠå¤©æ¨¡æ¿ï¼ˆæ–‡å­— tokenizationï¼‰
    text = processor.apply_chat_template(
        conversation,
        add_generation_prompt=True,
        tokenize=False
    )

    print(f"\nèŠå¤©æ¨¡æ¿è™•ç†å¾Œçš„æ–‡å­—æç¤º:")
    print(f"{text[:200]}...")  # é¡¯ç¤ºå‰200å€‹å­—ç¬¦

    # æå–éŸ³é »
    audios = []
    for message in conversation:
        if isinstance(message["content"], list):
            for ele in message["content"]:
                if ele["type"] == "audio":
                    audios.append(
                        librosa.load(
                            BytesIO(urlopen(ele['audio_url']).read()),
                            sr=processor.feature_extractor.sampling_rate
                        )[0]
                    )

    # çµ±ä¸€è™•ç†æ–‡å­—å’ŒéŸ³é »
    print("\næ­£åœ¨è™•ç†å¤šæ¨¡æ…‹è¼¸å…¥ï¼ˆæ–‡å­— + éŸ³é »ï¼‰...")
    inputs = processor(text=text, audios=audios, return_tensors="pt", padding=True)

    print(f"\nè™•ç†å¾Œçš„è¼¸å…¥:")
    print(f"  - input_ids å½¢ç‹€: {inputs.input_ids.shape}")
    print(f"  - input_features å½¢ç‹€: {inputs.input_features.shape if 'input_features' in inputs else 'N/A'}")
    print(f"  - attention_mask å½¢ç‹€: {inputs.attention_mask.shape}")

    # é¡¯ç¤ºç‰¹æ®ŠéŸ³é » tokens
    print("\nç‰¹æ®ŠéŸ³é » Tokens:")
    special_tokens = ["<|audio_bos|>", "<|AUDIO|>", "<|audio_eos|>"]
    for token in special_tokens:
        token_id = processor.tokenizer.convert_tokens_to_ids(token)
        print(f"  {token}: ID = {token_id}")

    # ============================================================
    # 5. ç”Ÿæˆå›æ‡‰
    # ============================================================
    print("\n" + "=" * 80)
    print("[æ­¥é©Ÿ 5] æ¨¡å‹ç”Ÿæˆå›æ‡‰")
    print("=" * 80)

    print("\næ­£åœ¨ç”Ÿæˆå›æ‡‰...")
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    with torch.no_grad():
        generate_ids = model.generate(**inputs, max_length=256)
        generate_ids = generate_ids[:, inputs['input_ids'].size(1):]

    response = processor.batch_decode(
        generate_ids,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=False
    )[0]

    print(f"\nğŸ¤– æ¨¡å‹å›æ‡‰: {response}")

    # ============================================================
    # 6. ç¸½çµ
    # ============================================================
    print("\n" + "=" * 80)
    print("ç¸½çµ - Qwen2-Audio Tokenization æ¶æ§‹")
    print("=" * 80)
    print("""
    ğŸ“ æ–‡å­—è™•ç†:
       - Tokenizer: Byte-level BPE (ä¾†è‡ª Qwen2)
       - è©å½™è¡¨: 151,000 tokens
       - åŠŸèƒ½: apply_chat_template(), encode(), decode()

    ğŸµ éŸ³é »è™•ç†:
       - Feature Extractor: Whisper-large-v3 based
       - æ¡æ¨£ç‡: 16kHz
       - ç‰¹å¾µ: 128-channel mel-spectrogram
       - ç‰¹æ®Š tokens: <|audio_bos|>, <|AUDIO|>, <|audio_eos|>

    ğŸ”— çµ±ä¸€è™•ç†:
       - AutoProcessor æ•´åˆå…©ç¨®æ¨¡æ…‹
       - processor(text=..., audios=...) çµ±ä¸€ä»‹é¢
    """)

if __name__ == "__main__":
    main()
