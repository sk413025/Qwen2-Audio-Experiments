"""
åŸºæ–¼ Transformer çš„ Domain Adapter å¯¦ç¾

éˆæ„Ÿä¾†æºï¼šsequence_insertion_explained.py
æ ¸å¿ƒæ¦‚å¿µï¼šä½¿ç”¨åºåˆ—ç´šçš„ Transformer ä¾†å­¸ç¿’ LDV â†’ éº¥å…‹é¢¨ çš„ç‰¹å¾µè½‰æ›

è¨­è¨ˆç†å¿µï¼š
1. ä¸æ”¹è®Šç‰¹å¾µç¶­åº¦ (1280ç¶­ä¿æŒä¸è®Š)
2. åœ¨åºåˆ—å±¤é¢é€²è¡Œé©æ‡‰ (è™•ç†æ™‚åºé—œä¿‚)
3. ä½¿ç”¨ Self-Attention æ•æ‰éŸ³é »å…§éƒ¨çš„ä¾è³´é—œä¿‚
4. æ®˜å·®é€£æ¥ä¿æŒåŸå§‹ä¿¡æ¯

æ¶æ§‹ï¼š
LDVéŸ³é » â†’ audio_tower â†’ [1, seq_len, 1280] â†’ Domain Adapter (Transformer) â†’ [1, seq_len, 1280] â†’ projector
                                              â†‘
                                    ä¿æŒç¶­åº¦ï¼Œåªèª¿æ•´ç‰¹å¾µåˆ†ä½ˆ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from transformers import AutoProcessor, Qwen2AudioForConditionalGeneration
import numpy as np
import librosa
from pathlib import Path
import json
import time
from typing import List, Dict, Optional

# ============================================================================
# ç¬¬ä¸€éƒ¨åˆ†ï¼šåŸºæ–¼ Transformer çš„ Domain Adapter
# ============================================================================

class TransformerDomainAdapter(nn.Module):
    """
    åŸºæ–¼ Transformer çš„ Domain Adapter

    è¨­è¨ˆéˆæ„Ÿä¾†è‡ªåºåˆ—æ’å…¥æ©Ÿåˆ¶ï¼š
    - åºåˆ—æ’å…¥ï¼šåœ¨åºåˆ—ç¶­åº¦æ“ä½œï¼Œä¿æŒç‰¹å¾µç¶­åº¦
    - Domain Adapterï¼šåŒæ¨£åœ¨åºåˆ—ç¶­åº¦å­¸ç¿’è½‰æ›ï¼Œä¿æŒç‰¹å¾µç¶­åº¦

    ç‚ºä»€éº¼ä½¿ç”¨ Transformerï¼Ÿ
    1. éŸ³é »æ˜¯æ™‚åºæ•¸æ“šï¼Œä¸åŒæ™‚é–“æ­¥ä¹‹é–“æœ‰ä¾è³´é—œä¿‚
    2. Self-Attention å¯ä»¥æ•æ‰é•·è·é›¢ä¾è³´
    3. å°±åƒåºåˆ—æ’å…¥å¾Œï¼Œattention è®“éŸ³é » embeddings äº’ç›¸é—œè¯
    4. Transformer å¯ä»¥å­¸ç¿’ã€Œå“ªäº›æ™‚é–“æ­¥éœ€è¦èª¿æ•´ï¼Œå“ªäº›ä¸éœ€è¦ã€

    å·¥ä½œåŸç†ï¼š
    ```
    è¼¸å…¥: [batch, seq_len, 1280]  (LDV ç‰¹å¾µ)
      â†“
    Position Encoding (æ·»åŠ ä½ç½®ä¿¡æ¯)
      â†“
    N Ã— Transformer Encoder Layer
      â”œâ”€ Multi-Head Self-Attention  (æ•æ‰æ™‚åºé—œä¿‚)
      â””â”€ Feed-Forward Network        (ç‰¹å¾µè½‰æ›)
      â†“
    è¼¸å‡º: [batch, seq_len, 1280]  (éº¥å…‹é¢¨é¢¨æ ¼ç‰¹å¾µ)

    + æ®˜å·®é€£æ¥: output = input + scale * adapter_output
    ```
    """

    def __init__(
        self,
        input_dim: int = 1280,          # audio_tower è¼¸å‡ºç¶­åº¦
        num_layers: int = 2,            # Transformer å±¤æ•¸
        num_heads: int = 8,             # æ³¨æ„åŠ›é ­æ•¸
        ff_dim: int = 2048,             # Feed-forward éš±è—å±¤ç¶­åº¦
        dropout: float = 0.1,           # Dropout æ¯”ç‡
        max_seq_len: int = 3000,        # æœ€å¤§åºåˆ—é•·åº¦
    ):
        super().__init__()

        self.input_dim = input_dim
        self.num_layers = num_layers

        # ä½ç½®ç·¨ç¢¼ï¼ˆé‡è¦ï¼éŸ³é »æ˜¯æ™‚åºæ•¸æ“šï¼‰
        # é¡ä¼¼æ–¼åºåˆ—æ’å…¥å¾Œï¼Œæ¯å€‹ä½ç½®éƒ½æœ‰ä½ç½®ä¿¡æ¯
        self.pos_encoding = nn.Parameter(
            torch.zeros(1, max_seq_len, input_dim)
        )
        nn.init.normal_(self.pos_encoding, std=0.02)

        # Transformer Encoder Layers
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=input_dim,
            nhead=num_heads,
            dim_feedforward=ff_dim,
            dropout=dropout,
            activation='gelu',
            batch_first=True,  # [batch, seq, features]
            norm_first=True    # Pre-LayerNorm (æ›´ç©©å®š)
        )

        self.transformer = nn.TransformerEncoder(
            encoder_layer,
            num_layers=num_layers
        )

        # è¼¸å‡ºæŠ•å½±ï¼ˆå¯é¸ï¼Œä¿æŒç¶­åº¦ä¸è®Šï¼‰
        self.output_proj = nn.Sequential(
            nn.Linear(input_dim, input_dim),
            nn.LayerNorm(input_dim)
        )

        # å¯å­¸ç¿’çš„ç¸®æ”¾å› å­ï¼ˆæ§åˆ¶ adapter çš„å½±éŸ¿ç¨‹åº¦ï¼‰
        # åˆå§‹åŒ–ç‚ºå°å€¼ï¼Œé¿å…ä¸€é–‹å§‹å°±ç ´å£åŸå§‹ç‰¹å¾µ
        self.scale = nn.Parameter(torch.ones(1) * 0.1)

        # Gate æ©Ÿåˆ¶ï¼ˆå¯é¸ï¼‰ï¼šè®“æ¨¡å‹å­¸ç¿’ã€Œæ¯å€‹æ™‚é–“æ­¥æ˜¯å¦éœ€è¦èª¿æ•´ã€
        self.use_gate = True
        if self.use_gate:
            self.gate = nn.Sequential(
                nn.Linear(input_dim, input_dim),
                nn.Sigmoid()
            )

    def forward(
        self,
        audio_features: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        å‰å‘å‚³æ’­

        Args:
            audio_features: [batch_size, seq_len, input_dim]
                           ä¾†è‡ª audio_tower çš„ç‰¹å¾µ
            attention_mask: [batch_size, seq_len]
                           å¯é¸çš„æ³¨æ„åŠ› mask

        Returns:
            adapted_features: [batch_size, seq_len, input_dim]
                            èª¿æ•´å¾Œçš„ç‰¹å¾µ

        æµç¨‹é¡æ¯”åºåˆ—æ’å…¥ï¼š
        1. å°±åƒæ–‡å­— tokens æœ‰ä½ç½®ç·¨ç¢¼ï¼ŒéŸ³é »ç‰¹å¾µä¹Ÿéœ€è¦ä½ç½®ä¿¡æ¯
        2. Self-Attention è®“æ¯å€‹æ™‚é–“æ­¥é—œæ³¨å…¶ä»–æ™‚é–“æ­¥ï¼ˆé¡ä¼¼æ–¼ attention è®“éŸ³é » embeddings äº’ç›¸é—œè¯ï¼‰
        3. å­¸ç¿’ç‰¹å¾µèª¿æ•´ï¼Œä½†ä¿æŒç¶­åº¦ä¸è®Š
        """
        batch_size, seq_len, _ = audio_features.shape

        # 1. æ·»åŠ ä½ç½®ç·¨ç¢¼
        # é¡ä¼¼æ–¼åºåˆ—æ’å…¥å¾Œï¼Œæ¯å€‹ä½ç½®éƒ½æœ‰æ¸…æ™°çš„ä½ç½®ä¿¡æ¯
        pos_enc = self.pos_encoding[:, :seq_len, :]
        features_with_pos = audio_features + pos_enc

        # 2. æº–å‚™ attention maskï¼ˆå¦‚æœæœ‰ï¼‰
        # PyTorch Transformer æœŸæœ› mask æ ¼å¼ï¼š[seq_len, seq_len]
        if attention_mask is not None:
            # å°‡ [batch, seq_len] è½‰æ›ç‚º [batch, seq_len, seq_len]
            # True è¡¨ç¤ºè¢« maskï¼ˆä¸é—œæ³¨ï¼‰ï¼ŒFalse è¡¨ç¤ºæ­£å¸¸é—œæ³¨
            key_padding_mask = (attention_mask == 0)
        else:
            key_padding_mask = None

        # 3. é€šé Transformer
        # Self-Attention è®“æ¨¡å‹å­¸ç¿’ï¼š
        # - å“ªäº›æ™‚é–“æ­¥éœ€è¦åƒè€ƒå…¶ä»–æ™‚é–“æ­¥
        # - å¦‚ä½•èª¿æ•´ç‰¹å¾µåˆ†ä½ˆ
        # - LDV å’Œéº¥å…‹é¢¨ç‰¹å¾µçš„å·®ç•°æ¨¡å¼
        transformed = self.transformer(
            features_with_pos,
            src_key_padding_mask=key_padding_mask
        )

        # 4. è¼¸å‡ºæŠ•å½±
        adapter_output = self.output_proj(transformed)

        # 5. Gate æ©Ÿåˆ¶ï¼ˆå¯é¸ï¼‰
        if self.use_gate:
            gate_values = self.gate(audio_features)
            adapter_output = gate_values * adapter_output

        # 6. æ®˜å·®é€£æ¥ + ç¸®æ”¾
        # ä¿ç•™åŸå§‹ä¿¡æ¯ï¼Œåªæ·»åŠ å¿…è¦çš„èª¿æ•´
        # output = input + scale * Î”
        adapted_features = audio_features + self.scale * adapter_output

        return adapted_features

    def get_num_params(self):
        """è¨ˆç®—åƒæ•¸é‡"""
        return sum(p.numel() for p in self.parameters())


class LightweightDomainAdapter(nn.Module):
    """
    è¼•é‡ç´š Domain Adapterï¼ˆæ›´å°çš„ç‰ˆæœ¬ï¼‰

    ä½¿ç”¨å ´æ™¯ï¼š
    - æ•¸æ“šè¼ƒå°‘æ™‚
    - è¨ˆç®—è³‡æºæœ‰é™æ™‚
    - LDV å’Œéº¥å…‹é¢¨å·®ç•°è¼ƒå°æ™‚

    æ¶æ§‹ç°¡åŒ–ï¼š
    - åªæœ‰ 1 å±¤ Transformer
    - è¼ƒå°‘çš„ attention heads
    - è¼ƒå°çš„ FFN
    """

    def __init__(
        self,
        input_dim: int = 1280,
        num_heads: int = 4,
        ff_dim: int = 1024,
        dropout: float = 0.1,
    ):
        super().__init__()

        self.input_dim = input_dim

        # ç°¡åŒ–çš„å–®å±¤ Transformer
        self.attention = nn.MultiheadAttention(
            embed_dim=input_dim,
            num_heads=num_heads,
            dropout=dropout,
            batch_first=True
        )

        self.norm1 = nn.LayerNorm(input_dim)

        self.ffn = nn.Sequential(
            nn.Linear(input_dim, ff_dim),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(ff_dim, input_dim),
            nn.Dropout(dropout)
        )

        self.norm2 = nn.LayerNorm(input_dim)

        self.scale = nn.Parameter(torch.ones(1) * 0.1)

    def forward(self, audio_features: torch.Tensor) -> torch.Tensor:
        """å‰å‘å‚³æ’­"""
        # Self-attention
        attn_out, _ = self.attention(
            audio_features, audio_features, audio_features
        )
        features = self.norm1(audio_features + attn_out)

        # Feed-forward
        ffn_out = self.ffn(features)
        features = self.norm2(features + ffn_out)

        # æ®˜å·®é€£æ¥
        return audio_features + self.scale * features

    def get_num_params(self):
        """è¨ˆç®—åƒæ•¸é‡"""
        return sum(p.numel() for p in self.parameters())


# ============================================================================
# ç¬¬äºŒéƒ¨åˆ†ï¼šå°‡ Domain Adapter æ’å…¥ Qwen2-Audio
# ============================================================================

class Qwen2AudioWithDomainAdapter(nn.Module):
    """
    å¸¶æœ‰ Domain Adapter çš„ Qwen2-Audio

    æ¶æ§‹ï¼š
    LDVéŸ³é » â†’ audio_tower â†’ [Domain Adapter] â†’ projector â†’ LLM
                           â†‘ æ’å…¥åœ¨é€™è£¡

    é¡æ¯”åºåˆ—æ’å…¥ï¼š
    - åºåˆ—æ’å…¥ï¼šåœ¨ç‰¹å®šä½ç½®æ’å…¥éŸ³é » embeddings
    - Domain Adapterï¼šåœ¨ç‰¹å®šä½ç½®æ’å…¥ç‰¹å¾µè½‰æ›æ¨¡çµ„
    """

    def __init__(
        self,
        base_model: Qwen2AudioForConditionalGeneration,
        adapter_type: str = "transformer",  # "transformer" or "lightweight"
        **adapter_kwargs
    ):
        super().__init__()

        self.base_model = base_model

        # å‰µå»º Domain Adapter
        if adapter_type == "transformer":
            self.domain_adapter = TransformerDomainAdapter(**adapter_kwargs)
        elif adapter_type == "lightweight":
            self.domain_adapter = LightweightDomainAdapter(**adapter_kwargs)
        else:
            raise ValueError(f"Unknown adapter type: {adapter_type}")

        # å‡çµåŸå§‹æ¨¡å‹çš„æ‰€æœ‰åƒæ•¸
        for param in self.base_model.parameters():
            param.requires_grad = False

        # åªè¨“ç·´ domain adapter
        for param in self.domain_adapter.parameters():
            param.requires_grad = True

        print(f"\nâœ“ Domain Adapter å·²å‰µå»º")
        print(f"  é¡å‹: {adapter_type}")
        print(f"  åƒæ•¸é‡: {self.domain_adapter.get_num_params():,}")

    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: torch.Tensor,
        input_features: Optional[torch.Tensor] = None,
        feature_attention_mask: Optional[torch.Tensor] = None,
        labels: Optional[torch.Tensor] = None,
        **kwargs
    ):
        """
        å‰å‘å‚³æ’­ï¼Œæ’å…¥ Domain Adapter

        æµç¨‹ï¼š
        1. å¦‚æœæœ‰éŸ³é »ç‰¹å¾µï¼Œé€šé audio_tower
        2. é€šé Domain Adapter èª¿æ•´ç‰¹å¾µ
        3. ç¹¼çºŒåŸå§‹æ¨¡å‹çš„æµç¨‹
        """

        # å¦‚æœæœ‰éŸ³é »ç‰¹å¾µï¼Œéœ€è¦å…ˆè™•ç†
        if input_features is not None:
            # 1. é€šé audio_tower
            # æ³¨æ„ï¼šé€™éœ€è¦è¨ªå• base_model çš„å…§éƒ¨
            # å¯¦éš›å¯¦ç¾å¯èƒ½éœ€è¦ä¿®æ”¹

            # ç²å– audio_tower
            audio_tower = self.base_model.audio_tower

            # æå–éŸ³é »ç‰¹å¾µ
            with torch.no_grad():  # audio_tower æ˜¯å‡çµçš„
                audio_features = audio_tower(input_features)

            # 2. é€šé Domain Adapterï¼ˆé—œéµæ­¥é©Ÿï¼ï¼‰
            audio_features = self.domain_adapter(
                audio_features,
                attention_mask=feature_attention_mask
            )

            # 3. å°‡èª¿æ•´å¾Œçš„ç‰¹å¾µå‚³çµ¦å¾ŒçºŒæµç¨‹
            # æ³¨æ„ï¼šé€™è£¡éœ€è¦æ›¿æ›åŸå§‹çš„éŸ³é »ç‰¹å¾µ
            # å¯¦éš›å¯¦ç¾ä¸­ï¼Œå¯èƒ½éœ€è¦ä¿®æ”¹ base_model çš„ forward
            # é€™è£¡ç°¡åŒ–è™•ç†

            # ç›´æ¥èª¿ç”¨ base_modelï¼Œè®“å®ƒè™•ç†å¾ŒçºŒæµç¨‹
            # æ³¨æ„ï¼šé€™æ˜¯ç°¡åŒ–ç‰ˆæœ¬ï¼Œå¯¦éš›éœ€è¦æ›´è¤‡é›œçš„é›†æˆ
            return self.base_model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                # å‚³å…¥èª¿æ•´å¾Œçš„éŸ³é »ç‰¹å¾µ
                # å¯¦éš›å¯¦ç¾å¯èƒ½éœ€è¦ä¸åŒçš„åƒæ•¸å
                labels=labels,
                **kwargs
            )
        else:
            # æ²’æœ‰éŸ³é »ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹æ¨¡å‹
            return self.base_model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels,
                **kwargs
            )

    def generate(self, **kwargs):
        """ç”Ÿæˆæ–¹æ³•"""
        return self.base_model.generate(**kwargs)


# ============================================================================
# ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ›´å¯¦ç”¨çš„é›†æˆæ–¹æ³•ï¼ˆMonkey Patchingï¼‰
# ============================================================================

def add_domain_adapter_to_model(
    model: Qwen2AudioForConditionalGeneration,
    adapter_type: str = "transformer",
    **adapter_kwargs
):
    """
    é€šé monkey patching å°‡ Domain Adapter æ·»åŠ åˆ°æ¨¡å‹

    é€™æ˜¯ä¸€å€‹æ›´å¯¦ç”¨çš„æ–¹æ³•ï¼Œä¸éœ€è¦é‡å¯«æ•´å€‹ forward

    Args:
        model: åŸå§‹çš„ Qwen2-Audio æ¨¡å‹
        adapter_type: adapter é¡å‹
        **adapter_kwargs: adapter çš„åƒæ•¸

    Returns:
        ä¿®æ”¹å¾Œçš„æ¨¡å‹
    """
    print("\n" + "="*100)
    print("  æ·»åŠ  Domain Adapterï¼ˆMonkey Patching æ–¹å¼ï¼‰")
    print("="*100)

    # 1. å‰µå»º Domain Adapter
    if adapter_type == "transformer":
        domain_adapter = TransformerDomainAdapter(**adapter_kwargs)
    elif adapter_type == "lightweight":
        domain_adapter = LightweightDomainAdapter(**adapter_kwargs)
    else:
        raise ValueError(f"Unknown adapter type: {adapter_type}")

    # 2. å°‡ adapter æ·»åŠ ç‚ºæ¨¡å‹çš„å±¬æ€§
    model.domain_adapter = domain_adapter

    # 3. ä¿å­˜åŸå§‹çš„ audio_tower forward
    original_audio_tower_forward = model.audio_tower.forward

    # 4. å®šç¾©æ–°çš„ forwardï¼Œæ’å…¥ Domain Adapter
    def audio_tower_with_adapter(self, input_features):
        """æ–°çš„ audio_tower forwardï¼ŒåŒ…å« Domain Adapter"""
        # èª¿ç”¨åŸå§‹çš„ audio_tower
        audio_features = original_audio_tower_forward(input_features)

        # é€šé Domain Adapter
        audio_features = model.domain_adapter(audio_features)

        return audio_features

    # 5. æ›¿æ› audio_tower çš„ forward æ–¹æ³•
    import types
    model.audio_tower.forward = types.MethodType(
        audio_tower_with_adapter,
        model.audio_tower
    )

    # 6. å‡çµæ‰€æœ‰åŸå§‹åƒæ•¸
    for name, param in model.named_parameters():
        if 'domain_adapter' not in name:
            param.requires_grad = False

    # 7. åªè¨“ç·´ domain adapter
    for param in model.domain_adapter.parameters():
        param.requires_grad = True

    # 8. çµ±è¨ˆåƒæ•¸
    trainable_params = sum(
        p.numel() for p in model.parameters() if p.requires_grad
    )
    total_params = sum(p.numel() for p in model.parameters())

    print(f"\nâœ“ Domain Adapter å·²æ·»åŠ ")
    print(f"  é¡å‹: {adapter_type}")
    print(f"  ä½ç½®: audio_tower â†’ [Domain Adapter] â†’ projector")
    print(f"  åƒæ•¸é‡: {domain_adapter.get_num_params():,} ({domain_adapter.get_num_params()/1e6:.2f}M)")
    print(f"\nåƒæ•¸çµ±è¨ˆ:")
    print(f"  ç¸½åƒæ•¸: {total_params:,} ({total_params/1e9:.2f}B)")
    print(f"  å¯è¨“ç·´: {trainable_params:,} ({trainable_params/1e6:.2f}M)")
    print(f"  æ¯”ä¾‹: {trainable_params/total_params*100:.4f}%")

    print("\næ¶æ§‹:")
    print("  LDVéŸ³é » â†’ audio_tower â†’ [Domain Adapter] â†’ projector â†’ LLM")
    print("                          â†‘ æ–°æ’å…¥çš„æ¨¡çµ„")
    print("="*100)

    return model


# ============================================================================
# ç¬¬å››éƒ¨åˆ†ï¼šè¨“ç·´ç›¸é—œï¼ˆé‡ç”¨ç¾æœ‰ä»£ç¢¼ï¼‰
# ============================================================================

# å¯ä»¥é‡ç”¨ stage1_lora_training.py ä¸­çš„ï¼š
# - Qwen2AudioTrainingDataset
# - load_dataset_from_disk
# - collate_fn
# - validate
# - train_with_lora (é‡å‘½åç‚º train_with_adapter)

# é€™è£¡ç°¡åŒ–ï¼Œåªå±•ç¤ºé—œéµå·®ç•°

def train_with_domain_adapter(
    model,
    processor,
    train_loader,
    val_loader=None,
    num_epochs=3,
    learning_rate=1e-4,
    device='mps'
):
    """
    ä½¿ç”¨ Domain Adapter è¨“ç·´

    èˆ‡ LoRA è¨“ç·´çš„å€åˆ¥ï¼š
    - LoRA: åœ¨ LLM å±¤æ·»åŠ  adapters
    - Domain Adapter: åœ¨ audio_tower å’Œ projector ä¹‹é–“æ·»åŠ  adapter

    ç›¸åŒé»ï¼š
    - éƒ½åªè¨“ç·´å°‘é‡åƒæ•¸
    - éƒ½ä¿ç•™åŸå§‹æ¨¡å‹ä¸è®Š
    - éƒ½ä½¿ç”¨ç›¸åŒçš„è¨“ç·´å¾ªç’°
    """
    print("\n" + "="*100)
    print("  é–‹å§‹ Domain Adapter è¨“ç·´")
    print("="*100)

    model = model.to(device)
    model.train()

    # å„ªåŒ–å™¨ï¼ˆåªå„ªåŒ– domain_adapterï¼‰
    trainable_params = [
        p for p in model.parameters() if p.requires_grad
    ]

    if len(trainable_params) == 0:
        print("âŒ éŒ¯èª¤ï¼šæ²’æœ‰å¯è¨“ç·´åƒæ•¸ï¼")
        return model

    optimizer = AdamW(trainable_params, lr=learning_rate, weight_decay=0.01)

    print(f"\nè¨“ç·´é…ç½®:")
    print(f"  è¨“ç·´æ¨£æœ¬: {len(train_loader.dataset)}")
    print(f"  Batch size: {train_loader.batch_size}")
    print(f"  Epochs: {num_epochs}")
    print(f"  å­¸ç¿’ç‡: {learning_rate}")
    print(f"  å¯è¨“ç·´åƒæ•¸: {sum(p.numel() for p in trainable_params):,}")
    print(f"  è¨­å‚™: {device}\n")

    # è¨“ç·´å¾ªç’°ï¼ˆèˆ‡ LoRA ç›¸åŒï¼‰
    for epoch in range(num_epochs):
        total_loss = 0
        start_time = time.time()

        for step, batch in enumerate(train_loader):
            try:
                input_ids = batch['input_ids'].to(device)
                attention_mask = batch['attention_mask'].to(device)
                labels = batch['labels'].to(device)

                input_features = batch.get('input_features')
                feature_attention_mask = batch.get('feature_attention_mask')
                if input_features is not None:
                    input_features = input_features.to(device)
                if feature_attention_mask is not None:
                    feature_attention_mask = feature_attention_mask.to(device)

                # Forward pass
                outputs = model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    input_features=input_features,
                    feature_attention_mask=feature_attention_mask,
                    labels=labels
                )

                loss = outputs.loss

                # Backward
                optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(trainable_params, max_norm=1.0)
                optimizer.step()

                total_loss += loss.item()

                if (step + 1) % 5 == 0 or step == 0:
                    avg_loss = total_loss / (step + 1)
                    print(f"Epoch [{epoch+1}/{num_epochs}], "
                          f"Step [{step+1}/{len(train_loader)}], "
                          f"Loss: {loss.item():.4f}, "
                          f"Avg Loss: {avg_loss:.4f}")

            except Exception as e:
                print(f"âŒ æ­¥é©Ÿ {step} å‡ºéŒ¯: {e}")
                import traceback
                traceback.print_exc()
                continue

        epoch_time = time.time() - start_time
        avg_epoch_loss = total_loss / len(train_loader)

        print(f"\n{'='*100}")
        print(f"Epoch {epoch+1}/{num_epochs} å®Œæˆ")
        print(f"  è¨“ç·´ Loss: {avg_epoch_loss:.4f}")
        print(f"  è€—æ™‚: {epoch_time:.2f} ç§’")
        print(f"{'='*100}\n")

        # ä¿å­˜ checkpoint
        if (epoch + 1) % 2 == 0:
            checkpoint_path = f'checkpoint_domain_adapter_epoch_{epoch+1}.pt'
            torch.save({
                'epoch': epoch,
                'domain_adapter_state_dict': model.domain_adapter.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': avg_epoch_loss,
            }, checkpoint_path)
            print(f"âœ“ Checkpoint å·²ä¿å­˜: {checkpoint_path}\n")

    print("ğŸ‰ Domain Adapter è¨“ç·´å®Œæˆï¼\n")
    return model


# ============================================================================
# ç¬¬äº”éƒ¨åˆ†ï¼šä¸»ç¨‹å¼
# ============================================================================

def main():
    """
    ä¸»ç¨‹å¼ï¼šå±•ç¤ºå¦‚ä½•ä½¿ç”¨åŸºæ–¼ Transformer çš„ Domain Adapter
    """

    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                                                                          â•‘
â•‘      åŸºæ–¼ Transformer çš„ Domain Adapter                                â•‘
â•‘                                                                          â•‘
â•‘      éˆæ„Ÿä¾†æºï¼šsequence_insertion_explained.py                          â•‘
â•‘                                                                          â•‘
â•‘      æ ¸å¿ƒæ¦‚å¿µï¼š                                                          â•‘
â•‘        â€¢ åºåˆ—æ’å…¥ï¼šåœ¨åºåˆ—ç¶­åº¦æ“ä½œï¼Œä¿æŒç‰¹å¾µç¶­åº¦                         â•‘
â•‘        â€¢ Domain Adapterï¼šåŒæ¨£åœ¨åºåˆ—ç¶­åº¦å­¸ç¿’è½‰æ›                        â•‘
â•‘        â€¢ ä½¿ç”¨ Transformer æ•æ‰æ™‚åºé—œä¿‚                                 â•‘
â•‘                                                                          â•‘
â•‘      æ¶æ§‹ï¼š                                                              â•‘
â•‘        LDVéŸ³é » â†’ audio_tower â†’ [Domain Adapter] â†’ projector â†’ LLM      â•‘
â•‘                                â†‘ Transformer è™•ç†                       â•‘
â•‘                                                                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    model_name = "Qwen/Qwen2-Audio-7B-Instruct"

    # ========================================================================
    # æ­¥é©Ÿ 1: åŠ è¼‰æ¨¡å‹
    # ========================================================================
    print("\n" + "="*100)
    print("  æ­¥é©Ÿ 1: åŠ è¼‰ Qwen2-Audio æ¨¡å‹")
    print("="*100)

    device = 'mps' if torch.backends.mps.is_available() else 'cpu'
    print(f"\nä½¿ç”¨è¨­å‚™: {device}")

    print(f"\nåŠ è¼‰æ¨¡å‹: {model_name}")
    model = Qwen2AudioForConditionalGeneration.from_pretrained(
        model_name,
        torch_dtype=torch.float32,
        device_map=device,
        trust_remote_code=True
    )

    print("âœ“ æ¨¡å‹åŠ è¼‰å®Œæˆ")

    # ========================================================================
    # æ­¥é©Ÿ 2: æ·»åŠ  Domain Adapter
    # ========================================================================
    print("\n" + "="*100)
    print("  æ­¥é©Ÿ 2: æ·»åŠ  Transformer Domain Adapter")
    print("="*100)

    # é¸é … 1: å®Œæ•´ç‰ˆ Transformer Adapter
    model = add_domain_adapter_to_model(
        model,
        adapter_type="transformer",
        input_dim=1280,         # audio_tower è¼¸å‡ºç¶­åº¦
        num_layers=2,           # Transformer å±¤æ•¸
        num_heads=8,            # æ³¨æ„åŠ›é ­æ•¸
        ff_dim=2048,            # Feed-forward ç¶­åº¦
        dropout=0.1
    )

    # é¸é … 2: è¼•é‡ç´š Adapterï¼ˆæ•¸æ“šè¼ƒå°‘æ™‚ä½¿ç”¨ï¼‰
    # model = add_domain_adapter_to_model(
    #     model,
    #     adapter_type="lightweight",
    #     input_dim=1280,
    #     num_heads=4,
    #     ff_dim=1024,
    #     dropout=0.1
    # )

    # ========================================================================
    # æ­¥é©Ÿ 3: é©—è­‰è¨­ç½®
    # ========================================================================
    print("\n" + "="*100)
    print("  æ­¥é©Ÿ 3: é©—è­‰ Domain Adapter è¨­ç½®")
    print("="*100)

    # æª¢æŸ¥å¯è¨“ç·´åƒæ•¸
    trainable_names = []
    for name, param in model.named_parameters():
        if param.requires_grad:
            trainable_names.append(name)

    print(f"\nå¯è¨“ç·´åƒæ•¸:")
    for name in trainable_names[:10]:  # é¡¯ç¤ºå‰ 10 å€‹
        print(f"  - {name}")
    if len(trainable_names) > 10:
        print(f"  ... é‚„æœ‰ {len(trainable_names) - 10} å€‹")

    # ========================================================================
    # æ­¥é©Ÿ 4: æº–å‚™æ•¸æ“šä¸¦è¨“ç·´
    # ========================================================================
    print("\n" + "="*100)
    print("  æ­¥é©Ÿ 4: æº–å‚™è¨“ç·´ï¼ˆéœ€è¦çœŸå¯¦çš„ LDV æ•¸æ“šï¼‰")
    print("="*100)

    print("\nâš ï¸  æ³¨æ„ï¼šé€™å€‹è…³æœ¬å±•ç¤ºäº†æ¶æ§‹è¨­è¨ˆ")
    print("å¯¦éš›è¨“ç·´éœ€è¦ï¼š")
    print("  1. çœŸå¯¦çš„ LDV éŸ³é »æ•¸æ“šï¼ˆä¸æ˜¯åˆæˆæ•¸æ“šï¼‰")
    print("  2. å°æ‡‰çš„æ¨™è¨»ï¼ˆè½‰éŒ„æ–‡æœ¬æˆ–å°è©±ï¼‰")
    print("  3. å¦‚æœ‰éº¥å…‹é¢¨æ•¸æ“šå°æ¯”æ›´å¥½")

    print("\nå¦‚ä½•ä½¿ç”¨ï¼š")
    print("  1. æº–å‚™ LDV æ•¸æ“šé›†")
    print("  2. ä½¿ç”¨ stage1_lora_training.py çš„æ•¸æ“šåŠ è¼‰ä»£ç¢¼")
    print("  3. èª¿ç”¨ train_with_domain_adapter()")

    # ç¤ºä¾‹ï¼šå¦‚æœæœ‰æ•¸æ“š
    # train_loader = prepare_ldv_dataloader(...)
    # model = train_with_domain_adapter(
    #     model, processor, train_loader,
    #     num_epochs=3, learning_rate=1e-4, device=device
    # )

    print("\n" + "="*100)
    print("  æ¶æ§‹è¨­è¨ˆå®Œæˆ")
    print("="*100)

    print("\nâœ… Domain Adapter çš„å„ªå‹¢:")
    print("  1. ä½¿ç”¨ Transformer æ•æ‰éŸ³é »æ™‚åºé—œä¿‚")
    print("  2. é¡ä¼¼åºåˆ—æ’å…¥ï¼Œåœ¨åºåˆ—ç¶­åº¦æ“ä½œ")
    print("  3. ä¿æŒç‰¹å¾µç¶­åº¦ä¸è®Š (1280 â†’ 1280)")
    print("  4. å®Œå…¨ä¸ä¿®æ”¹åŸå§‹æ¨¡å‹")
    print("  5. å¯å­¸ç¿’çš„ç¸®æ”¾å’Œ gate æ©Ÿåˆ¶")

    print("\nèˆ‡åºåˆ—æ’å…¥çš„é¡æ¯”:")
    print("  åºåˆ—æ’å…¥:    åœ¨ä½ç½®ç¶­åº¦æ’å…¥éŸ³é » embeddings")
    print("  Domain Adapter: åœ¨æ™‚åºç¶­åº¦èª¿æ•´éŸ³é »ç‰¹å¾µ")
    print("  å…±åŒé»:     éƒ½ä¿æŒç‰¹å¾µç¶­åº¦ï¼Œéƒ½ç”¨ Transformer è™•ç†")

    print("\nä¸‹ä¸€æ­¥:")
    print("  1. æº–å‚™ LDV éŸ³é »æ•¸æ“š")
    print("  2. é‹è¡Œè¨“ç·´")
    print("  3. è©•ä¼°åœ¨ LDV å’Œéº¥å…‹é¢¨éŸ³é »ä¸Šçš„æ€§èƒ½")
    print("  4. èª¿æ•´ adapter åƒæ•¸ï¼ˆå±¤æ•¸ã€é ­æ•¸ç­‰ï¼‰")

    print("="*100)


if __name__ == "__main__":
    main()
